{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLuaAq3ur5YQ"
   },
   "source": [
    "# 1. Setting up Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vBfzfdj56Rw"
   },
   "source": [
    "## Setting up DNABERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVoeJm-32GQu",
    "outputId": "f8c22223-c96e-4c2e-bf75-0c74462b60f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/dnabert\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.6\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2016.9.26          |           py36_0         217 KB  conda-forge\n",
      "    libsqlite-3.40.0           |       h753d276_1         785 KB  conda-forge\n",
      "    pip-20.0.2                 |           py36_1         1.9 MB  conda-forge\n",
      "    python-3.6.15              |hb7a2778_0_cpython        38.4 MB  conda-forge\n",
      "    python_abi-3.6             |          2_cp36m           4 KB  conda-forge\n",
      "    setuptools-49.6.0          |   py36h5fab9bb_3         936 KB  conda-forge\n",
      "    sqlite-3.40.0              |       h4ff8645_1         773 KB  conda-forge\n",
      "    wheel-0.34.2               |           py36_0          43 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        43.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge None\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu None\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.12.7-ha878542_0 None\n",
      "  certifi            conda-forge/linux-64::certifi-2016.9.26-py36_0 None\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-h41732ed_0 None\n",
      "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 None\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19 None\n",
      "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19 None\n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 None\n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.40.0-h753d276_1 None\n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
      "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4 None\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1 None\n",
      "  openssl            conda-forge/linux-64::openssl-1.1.1t-h0b41bf4_0 None\n",
      "  pip                conda-forge/linux-64::pip-20.0.2-py36_1 None\n",
      "  python             conda-forge/linux-64::python-3.6.15-hb7a2778_0_cpython None\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-2_cp36m None\n",
      "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 None\n",
      "  setuptools         conda-forge/linux-64::setuptools-49.6.0-py36h5fab9bb_3 None\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.40.0-h4ff8645_1 None\n",
      "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 None\n",
      "  wheel              conda-forge/linux-64::wheel-0.34.2-py36_0 None\n",
      "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "sqlite-3.40.0        | 773 KB    | ##################################### | 100% \n",
      "certifi-2016.9.26    | 217 KB    | ##################################### | 100% \n",
      "python-3.6.15        | 38.4 MB   | ##################################### | 100% \n",
      "wheel-0.34.2         | 43 KB     | ##################################### | 100% \n",
      "pip-20.0.2           | 1.9 MB    | ##################################### | 100% \n",
      "python_abi-3.6       | 4 KB      | ##################################### | 100% \n",
      "setuptools-49.6.0    | 936 KB    | ##################################### | 100% \n",
      "libsqlite-3.40.0     | 785 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate dnabert\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "#conda remove --name dnabert --all -y\n",
    "!conda create -n dnabert python=3.6 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ciq0v3V7piR",
    "outputId": "9e2e0084-ac9f-4005-c1ca-7f1c4bcae72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DNABERT'...\n",
      "remote: Enumerating objects: 766, done.\u001b[K\n",
      "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 766 (delta 53), reused 50 (delta 50), pack-reused 700\u001b[K\n",
      "Receiving objects: 100% (766/766), 11.68 MiB | 23.91 MiB/s, done.\n",
      "Resolving deltas: 100% (409/409), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jerryji1993/DNABERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Conda Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/dnabert\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.2\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |              mkl           6 KB  intel\n",
      "    cffi-1.11.5                |           py36_3         411 KB  intel\n",
      "    cudatoolkit-10.2.89        |      h713d32c_11       318.0 MB  conda-forge\n",
      "    dataclasses-0.8            |     pyh787bdff_2          22 KB  conda-forge\n",
      "    icu-72.1                   |       hcb278e6_0        11.4 MB  conda-forge\n",
      "    intel-openmp-2023.1.0      |      intel_46305        18.5 MB  intel\n",
      "    intelpython-2023.1.0       |                1           8 KB  intel\n",
      "    lerc-3.0                   |       h295c915_0         223 KB  intel\n",
      "    libblas-3.9.0              |1_h86c2bf4_netlib         199 KB  conda-forge\n",
      "    libcblas-3.9.0             |5_h92ddd45_netlib          54 KB  conda-forge\n",
      "    libdeflate-1.8             |       h7f8727e_5          67 KB  intel\n",
      "    libhwloc-2.9.1             |       hd6dc26d_0         2.5 MB  conda-forge\n",
      "    liblapack-3.9.0            |5_h92ddd45_netlib         3.0 MB  conda-forge\n",
      "    libtiff-4.3.0              |       h6f004c6_2         614 KB  conda-forge\n",
      "    libxml2-2.10.4             |       hfdac1af_0         697 KB  conda-forge\n",
      "    mkl-2023.1.0               |      intel_46342       197.2 MB  intel\n",
      "    ninja-1.11.1               |       h924138e_0         2.1 MB  conda-forge\n",
      "    numpy-1.19.5               |   py36hfc0c790_2         5.3 MB  conda-forge\n",
      "    pillow-7.2.0               |   py36h8328e55_0         657 KB  conda-forge\n",
      "    pytorch-1.8.1              |py3.6_cuda10.2_cudnn7.6.5_0       673.3 MB  pytorch\n",
      "    pytorch-cpu-1.1.0          |      py3.6_cpu_0        53.5 MB  pytorch\n",
      "    tbb-2021.9.0               |       hf52228f_0         1.5 MB  conda-forge\n",
      "    torchvision-0.9.1          |py36h2de9e49_1_cpu         6.6 MB  conda-forge\n",
      "    typing_extensions-3.10.0.2 |     pyha770c72_0          28 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.27 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               intel/linux-64::blas-1.0-mkl None\n",
      "  cffi               intel/linux-64::cffi-1.11.5-py36_3 None\n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-10.2.89-h713d32c_11 None\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyh787bdff_2 None\n",
      "  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 None\n",
      "  icu                conda-forge/linux-64::icu-72.1-hcb278e6_0 None\n",
      "  intel-openmp       intel/linux-64::intel-openmp-2023.1.0-intel_46305 None\n",
      "  intelpython        intel/linux-64::intelpython-2023.1.0-1 None\n",
      "  jbig               conda-forge/linux-64::jbig-2.1-h7f98852_2003 None\n",
      "  jpeg               conda-forge/linux-64::jpeg-9e-h0b41bf4_3 None\n",
      "  lerc               intel/linux-64::lerc-3.0-h295c915_0 None\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-1_h86c2bf4_netlib None\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-5_h92ddd45_netlib None\n",
      "  libdeflate         intel/linux-64::libdeflate-1.8-h7f8727e_5 None\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
      "  libhwloc           conda-forge/linux-64::libhwloc-2.9.1-hd6dc26d_0 None\n",
      "  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 None\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-5_h92ddd45_netlib None\n",
      "  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 None\n",
      "  libtiff            conda-forge/linux-64::libtiff-4.3.0-h6f004c6_2 None\n",
      "  libuv              conda-forge/linux-64::libuv-1.44.2-h166bdaf_0 None\n",
      "  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.0-h0b41bf4_0 None\n",
      "  libxml2            conda-forge/linux-64::libxml2-2.10.4-hfdac1af_0 None\n",
      "  mkl                intel/linux-64::mkl-2023.1.0-intel_46342 None\n",
      "  ninja              conda-forge/linux-64::ninja-1.11.1-h924138e_0 None\n",
      "  numpy              conda-forge/linux-64::numpy-1.19.5-py36hfc0c790_2 None\n",
      "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1 None\n",
      "  pillow             conda-forge/linux-64::pillow-7.2.0-py36h8328e55_0 None\n",
      "  pycparser          conda-forge/noarch::pycparser-2.21-pyhd8ed1ab_0 None\n",
      "  pytorch            pytorch/linux-64::pytorch-1.8.1-py3.6_cuda10.2_cudnn7.6.5_0 None\n",
      "  pytorch-cpu        pytorch/linux-64::pytorch-cpu-1.1.0-py3.6_cpu_0 None\n",
      "  tbb                conda-forge/linux-64::tbb-2021.9.0-hf52228f_0 None\n",
      "  torchvision        conda-forge/linux-64::torchvision-0.9.1-py36h2de9e49_1_cpu None\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-3.10.0.2-pyha770c72_0 None\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.13-h166bdaf_4 None\n",
      "  zstd               conda-forge/linux-64::zstd-1.5.2-h3eb15da_6 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "intelpython-2023.1.0 | 8 KB      | ########## | 100% \n",
      "mkl-2023.1.0         | 197.2 MB  | ########## | 100% \n",
      "intel-openmp-2023.1. | 18.5 MB   | ########## | 100% \n",
      "cudatoolkit-10.2.89  | 318.0 MB  | ########## | 100% \n",
      "libxml2-2.10.4       | 697 KB    | ########## | 100% \n",
      "dataclasses-0.8      | 22 KB     | ########## | 100% \n",
      "cffi-1.11.5          | 411 KB    | ########## | 100% \n",
      "libblas-3.9.0        | 199 KB    | ########## | 100% \n",
      "tbb-2021.9.0         | 1.5 MB    | ########## | 100% \n",
      "liblapack-3.9.0      | 3.0 MB    | ########## | 100% \n",
      "numpy-1.19.5         | 5.3 MB    | ########## | 100% \n",
      "libdeflate-1.8       | 67 KB     | ########## | 100% \n",
      "typing_extensions-3. | 28 KB     | ########## | 100% \n",
      "blas-1.0             | 6 KB      | ########## | 100% \n",
      "pytorch-cpu-1.1.0    | 53.5 MB   | ########## | 100% \n",
      "pillow-7.2.0         | 657 KB    | ########## | 100% \n",
      "torchvision-0.9.1    | 6.6 MB    | ########## | 100% \n",
      "pytorch-1.8.1        | 673.3 MB  | ########## | 100% \n",
      "ninja-1.11.1         | 2.1 MB    | ########## | 100% \n",
      "lerc-3.0             | 223 KB    | ########## | 100% \n",
      "libtiff-4.3.0        | 614 KB    | ########## | 100% \n",
      "libcblas-3.9.0       | 54 KB     | ########## | 100% \n",
      "icu-72.1             | 11.4 MB   | ########## | 100% \n",
      "libhwloc-2.9.1       | 2.5 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "#setting up dnabert environment\n",
    "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
    "\n",
    "conda activate dnabert\n",
    "conda install pytorch torchvision cudatoolkit=10.2 -c pytorch -y\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Obtaining file:///home/ec2-user/SageMaker/DNABERT\n",
      "Collecting certifi\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting tokenizers==0.5.0\n",
      "  Downloading tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.23.10-py3-none-any.whl (132 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.5.5-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "Collecting botocore<1.27.0,>=1.26.10\n",
      "  Downloading botocore-1.26.10-py3-none-any.whl (8.8 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting importlib-resources; python_version < \"3.7\"\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=9ec050f7d09aa6b4eeaf7005382aee1ebddbcb83e8be177bcc2b4ffa93666629\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: certifi, numpy, tokenizers, jmespath, six, python-dateutil, urllib3, botocore, s3transfer, boto3, filelock, idna, charset-normalizer, requests, zipp, importlib-resources, tqdm, regex, sentencepiece, typing-extensions, importlib-metadata, click, joblib, sacremoses, transformers\n",
      "  Running setup.py develop for transformers\n",
      "Successfully installed boto3-1.23.10 botocore-1.26.10 certifi-2022.12.7 charset-normalizer-2.0.12 click-8.0.4 filelock-3.4.1 idna-3.4 importlib-metadata-4.8.3 importlib-resources-5.4.0 jmespath-0.10.0 joblib-1.1.1 numpy-1.19.5 python-dateutil-2.8.2 regex-2023.5.5 requests-2.27.1 s3transfer-0.5.2 sacremoses-0.0.53 sentencepiece-0.1.99 six-1.16.0 tokenizers-0.5.0 tqdm-4.64.1 transformers typing-extensions-4.1.1 urllib3-1.26.15 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate dnabert\n",
    "\n",
    "cd DNABERT\n",
    "python3 -m pip install --editable . --ignore-installed certifi\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting certifi\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting scikit-learn>=0.22.2\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0.tar.gz (99 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.12.2-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting biopython\n",
      "  Downloading biopython-1.79-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting pybedtools\n",
      "  Downloading pybedtools-0.9.0.tar.gz (12.5 MB)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting protobuf<4,>=3.8.0\n",
      "  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting wheel>=0.26\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "Collecting patsy>=0.5\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pysam\n",
      "  Downloading pysam-0.21.0.tar.gz (4.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: seqeval, pyahocorasick, pybedtools, pysam\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=6ce3ab40d18e2e1eaf4ee1318a79b2f5f74a9d31794d205e749fd97baa4579ca\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/39/29/36/1c4f7905c133e11748ca375960154964082d4fb03478323089\n",
      "  Building wheel for pyahocorasick (setup.py): started\n",
      "  Building wheel for pyahocorasick (setup.py): finished with status 'done'\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-2.0.0-cp36-cp36m-linux_x86_64.whl size=104242 sha256=c5c2a5752a18d0aca9babd2c14346247d1872eb0045f9f46769848e99f5a91fa\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f9/31/72/912070b7b08d0c8a281753136b25fbb535d847aafd7c5477a0\n",
      "  Building wheel for pybedtools (setup.py): started\n",
      "  Building wheel for pybedtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pybedtools: filename=pybedtools-0.9.0-cp36-cp36m-linux_x86_64.whl size=13635553 sha256=0d724b1b41f72300857620737d649788f6911476bc9a618a3736401aba83066d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ce/15/57/09a084714fcb312a5704ced7f73c3973120a7917f4ddcae63e\n",
      "  Building wheel for pysam (PEP 517): started\n",
      "  Building wheel for pysam (PEP 517): still running...\n",
      "  Building wheel for pysam (PEP 517): still running...\n",
      "  Building wheel for pysam (PEP 517): finished with status 'done'\n",
      "  Created wheel for pysam: filename=pysam-0.21.0-cp36-cp36m-linux_x86_64.whl size=10376800 sha256=6567ec03844e9bd62f8202541ca10f4818c078a1c5226eaa49c21e2dfd4e9112\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/cc/2e/2d/1b275e668e48decc205ed67bb2f176850b8dbec0485ac22565\n",
      "Successfully built seqeval pyahocorasick pybedtools pysam\n",
      "Installing collected packages: certifi, numpy, pyparsing, packaging, protobuf, tensorboardX, six, grpcio, tensorboard-data-server, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, urllib3, charset-normalizer, idna, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, wheel, dataclasses, werkzeug, typing-extensions, zipp, importlib-metadata, markdown, setuptools, absl-py, tensorboard-plugin-wit, tensorboard, threadpoolctl, joblib, scipy, scikit-learn, seqeval, pyahocorasick, pytz, python-dateutil, pandas, patsy, statsmodels, biopython, pysam, pybedtools, sentencepiece\n",
      "Successfully installed absl-py-1.4.0 biopython-1.79 cachetools-4.2.4 certifi-2022.12.7 charset-normalizer-2.0.12 dataclasses-0.8 google-auth-2.17.3 google-auth-oauthlib-0.4.6 grpcio-1.48.2 idna-3.4 importlib-metadata-4.8.3 joblib-1.1.1 markdown-3.3.7 numpy-1.19.5 oauthlib-3.2.2 packaging-21.3 pandas-1.1.5 patsy-0.5.3 protobuf-3.19.6 pyahocorasick-2.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pybedtools-0.9.0 pyparsing-3.0.9 pysam-0.21.0 python-dateutil-2.8.2 pytz-2023.3 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-0.24.2 scipy-1.5.4 sentencepiece-0.1.99 seqeval-1.2.2 setuptools-59.6.0 six-1.16.0 statsmodels-0.12.2 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6 threadpoolctl-3.1.0 typing-extensions-4.1.1 urllib3-1.26.15 werkzeug-2.0.3 wheel-0.37.1 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate dnabert\n",
    "\n",
    "cd ./DNABERT/examples\n",
    "python3 -m pip install -r requirements.txt --ignore-installed certifi\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch==1.10.2+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.2%2Bcu113-cp36-cp36m-linux_x86_64.whl (1821.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1821.5 MB 1.9 kB/s eta 0:00:0102  |▏                               | 11.2 MB 83.2 MB/s eta 0:00:22     |███████████▍                    | 651.3 MB 119.0 MB/s eta 0:00:10███████████████████▋            | 1116.0 MB 680 kB/s eta 0:17:17��█████           | 1191.5 MB 1.3 MB/s eta 0:07:54     |██████████████████████          | 1256.9 MB 873 kB/s eta 0:10:47| 1451.6 MB 701 kB/s eta 0:08:48��█████████████▍| 1787.8 MB 1.4 MB/s eta 0:00:24\n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/dnabert/lib/python3.6/site-packages (from torch==1.10.2+cu113) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/dnabert/lib/python3.6/site-packages (from torch==1.10.2+cu113) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.1\n",
      "    Uninstalling torch-1.8.1:\n",
      "      Successfully uninstalled torch-1.8.1\n",
      "Successfully installed torch-1.10.2+cu113\n"
     ]
    }
   ],
   "source": [
    "!/home/ec2-user/anaconda3/envs/dnabert/bin/pip install torch==1.10.2+cu113 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtdNv2U877wH"
   },
   "source": [
    "### Downloading Pre-saved DNABERT Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "Amu01ato6v9T",
    "outputId": "62b7fbba-777e-4124-a5c5-7137432554d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.7.1\n",
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1V7CChcC6KgdJ7Gwdyn73OS6dZR_J-Lrs\n",
      "From (redirected): https://drive.google.com/uc?id=1V7CChcC6KgdJ7Gwdyn73OS6dZR_J-Lrs&confirm=t&uuid=0cfceb60-0d9d-4c41-aea5-1283fb4b2913\n",
      "To: /home/ec2-user/SageMaker/4-new-12w-0.zip\n",
      "100%|████████████████████████████████████████| 322M/322M [00:05<00:00, 64.0MB/s]\n",
      "Archive:  ./4-new-12w-0.zip\n",
      "   creating: 4-new-12w-0/\n",
      " extracting: 4-new-12w-0/tokenizer_config.json  \n",
      "  inflating: 4-new-12w-0/vocab.txt   \n",
      "  inflating: 4-new-12w-0/pytorch_model.bin  \n",
      "  inflating: 4-new-12w-0/special_tokens_map.json  \n",
      "  inflating: 4-new-12w-0/config.json  \n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1V7CChcC6KgdJ7Gwdyn73OS6dZR_J-Lrs/view?usp=sharing\n",
    "!unzip ./4-new-12w-0.zip\n",
    "!rm ./4-new-12w-0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I38sFnv08pBX"
   },
   "source": [
    "## Setting up BERT-PCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94G_CL0J8saa",
    "outputId": "af34b5af-16e8-4ac7-cf7d-c974bce5c932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert-pcp'...\n",
      "remote: Enumerating objects: 107, done.\u001b[K\n",
      "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
      "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
      "remote: Total 107 (delta 67), reused 70 (delta 33), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (107/107), 30.56 KiB | 15.28 MiB/s, done.\n",
      "Resolving deltas: 100% (67/67), done.\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf bert-pcp\n",
    "!git clone https://github.com/haydenji0731/bert-pcp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOtQrXxW9UrF",
    "outputId": "c6f0afe5-4cab-46f6-cbce-c3a4da0e58d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==2.0.1\n",
      "  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./bert-pcp/requirements.txt (line 3)) (2.8.2)\n",
      "Collecting pytz==2023.3\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./bert-pcp/requirements.txt (line 5)) (1.16.0)\n",
      "Collecting tzdata==2023.3\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.7\n",
      "    Uninstalling pytz-2022.7:\n",
      "      Successfully uninstalled pytz-2022.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Uninstalling numpy-1.22.3:\n",
      "      Successfully uninstalled numpy-1.22.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.2\n",
      "    Uninstalling pandas-1.5.2:\n",
      "      Successfully uninstalled pandas-1.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.20.3 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 pandas-2.0.1 pytz-2023.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!mkdir './bert-pcp/data' './bert-pcp/ft' './bert-pcp/results'\n",
    "!pip install -r './bert-pcp/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8spfCewAzgD"
   },
   "source": [
    "# 2. Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by decompressing the FASTA file containing transcript sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d \"./intersection.chess3.gencode.refseq.adjstop.noselenop.nodup.fa.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain train/val/test splits in .tsv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-SA7UZLA0Uq",
    "outputId": "26739697-ef76-43ad-fff4-0851c1f7c5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:37:42 | Loading annotation file.\n",
      "2023-05-04 01:37:43 | Starting kmer-izing sequences\n",
      "2023-05-04 01:38:15 | Kmer-ization completed. Extracting train/val/test sets.\n",
      "2023-05-04 01:38:48 | Pre-processing completed. Proceed with model training.\n"
     ]
    }
   ],
   "source": [
    "!python './bert-pcp/process_data_byG.py' -in_fasta './intersection.chess3.gencode.refseq.adjstop.noselenop.nodup.fa' \\\n",
    "  -in_gtf './intersection.chess3.gencode.refseq.gtf' -label './labels.txt' \\\n",
    "  -out_dir './bert-pcp/data/kmer4/' --kmer_size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S2khc-iA72h"
   },
   "source": [
    "# 3. Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S4S3m399CPU_"
   },
   "outputs": [],
   "source": [
    "!chmod +x './bert-pcp/train.sh'\n",
    "!chmod +x './bert-pcp/test.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0TAJqSvK5dB"
   },
   "source": [
    "## Train w/ BERT-PCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cK0_EtgtCvpK",
    "outputId": "aa0d8534-3053-43f1-b565-a587566138c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if the output directory is present\n",
      "output directory not found; making it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 01:39:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/04/2023 01:39:41 - INFO - transformers.configuration_utils -   loading configuration file ../4-new-12w-0/config.json\n",
      "05/04/2023 01:39:41 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 01:39:41 - INFO - transformers.file_utils -   https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt not found in cache or force_download set to True, downloading to /home/ec2-user/.cache/torch/transformers/tmplrezgx9u\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 1.31kB [00:00, 1.21MB/s]                 \n",
      "05/04/2023 01:39:41 - INFO - transformers.file_utils -   storing https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt in cache at /home/ec2-user/.cache/torch/transformers/7e2907c40805f9ae104ef14f1bb0e1d375c6edddace059a06037bdc80e8abe91.29586c1860b95c4db60b7024d64161b951279c64e0d9eeea911f286be8f229ae\n",
      "05/04/2023 01:39:41 - INFO - transformers.file_utils -   creating metadata file for /home/ec2-user/.cache/torch/transformers/7e2907c40805f9ae104ef14f1bb0e1d375c6edddace059a06037bdc80e8abe91.29586c1860b95c4db60b7024d64161b951279c64e0d9eeea911f286be8f229ae\n",
      "05/04/2023 01:39:41 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt from cache at /home/ec2-user/.cache/torch/transformers/7e2907c40805f9ae104ef14f1bb0e1d375c6edddace059a06037bdc80e8abe91.29586c1860b95c4db60b7024d64161b951279c64e0d9eeea911f286be8f229ae\n",
      "05/04/2023 01:39:41 - INFO - transformers.modeling_utils -   loading weights file ../4-new-12w-0/pytorch_model.bin\n",
      "05/04/2023 01:39:45 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/04/2023 01:39:45 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "05/04/2023 01:39:45 - INFO - __main__ -   finish loading model\n",
      "05/04/2023 01:39:49 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='./data/kmer4/train/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=False, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=1e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=100, max_steps=-1, model_name_or_path='../4-new-12w-0/', model_type='dna', n_gpu=1, n_process=4, no_cuda=False, num_rnn_layer=2, num_train_epochs=3.0, output_dir='./ft/kmer4/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=128, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=128, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=100000000000000000000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna4', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)\n",
      "05/04/2023 01:39:49 - INFO - __main__ -   Creating features from dataset file at ./data/kmer4/train/\n",
      "05/04/2023 01:39:49 - INFO - transformers.data.processors.glue -   LOOKING AT ./data/kmer4/train/train.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading examples\n",
      "number of processes for converting feature: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   Writing example 0/15994\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 68 259 254 234 155 95 110 172 164 131 254 236 163 125 232 146 60 228 131 254 236 163 126 236 163 127 239 175 173 166 139 31 111 173 168 148 68 259 255 240 179 192 244 196 260 259 255 239 176 180 196 258 250 219 95 109 167 142 42 156 100 131 256 244 195 256 242 187 222 106 155 94 107 158 107 160 113 184 210 59 224 115 191 238 170 155 96 115 190 236 163 126 235 159 112 177 184 209 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 52 193 247 207 46 169 151 79 45 167 142 44 164 131 255 239 174 172 163 128 242 185 214 76 34 124 228 130 252 227 125 232 147 63 238 171 158 107 158 106 155 93 101 134 11 31 111 174 169 149 72 17 53 199 14 44 164 130 250 219 94 106 154 91 93 101 133 8 19 61 230 140 34 121 214 74 26 90 91 94 108 161 117 200 17 53 197 7 14 42 154 89 88 81 53 199 15 45 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 259 255 238 172 164 129 247 208 51 190 234 154 92 100 131 253 229 135 15 48 179 190 233 151 78 43 159 111 176 180 196 260 260 258 252 227 126 234 154 90 90 91 94 108 163 125 232 148 68 257 247 208 49 181 200 18 60 227 127 237 167 143 46 169 150 76 35 126 233 152 82 60 228 131 256 244 196 258 251 222 108 164 129 245 200 19 63 238 169 152 81 56 212 68 260 257 245 199 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 41 152 83 64 243 192 244 195 256 244 195 255 240 179 191 240 180 195 255 239 176 179 189 229 134 12 36 130 252 227 126 233 151 79 47 174 172 164 130 250 220 99 126 236 163 126 235 160 113 184 209 55 206 44 163 128 243 192 243 189 232 148 68 259 256 244 194 251 223 110 171 160 116 196 258 251 223 112 179 192 244 193 248 212 67 254 236 163 128 243 190 235 158 108 163 128 243 191 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-5\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 139 30 107 158 106 156 97 119 207 48 180 196 260 258 252 226 122 219 94 107 157 103 143 45 166 140 35 127 239 174 170 154 91 94 105 151 77 37 133 7 13 37 133 5 5 5 5 6 12 33 117 200 17 54 204 34 124 228 129 248 209 55 207 46 172 164 129 248 209 55 205 40 148 66 252 228 131 253 231 143 46 171 160 114 188 227 126 234 155 95 110 171 159 112 177 181 200 20 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   Writing example 0/15994\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-15995\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 107 158 107 158 106 154 90 91 95 111 176 179 191 240 178 187 223 109 167 142 44 163 127 239 173 165 136 17 54 204 34 124 228 129 245 197 8 20 65 245 197 8 17 53 200 20 67 255 237 165 136 20 68 260 257 245 200 17 53 200 20 66 252 228 131 254 235 159 112 180 195 255 239 175 174 172 163 126 236 162 123 222 106 156 97 117 200 17 53 200 19 61 232 148 65 248 212 67 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-15996\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 209 53 199 15 46 170 153 86 73 22 74 28 98 122 219 94 108 161 117 200 17 56 209 56 212 66 252 228 130 252 225 119 206 42 153 85 69 5 5 6 11 29 102 140 35 126 235 157 101 134 9 24 84 65 246 202 25 87 80 51 190 236 161 120 212 67 255 239 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   guid: train-15997\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   input_ids: 2 39 142 42 153 87 79 46 172 164 131 253 232 148 68 260 257 248 209 54 201 24 82 57 214 76 33 118 203 29 102 140 33 117 197 8 18 60 228 130 250 218 90 90 91 95 109 168 145 56 211 64 241 184 212 67 254 234 153 86 75 31 109 166 138 28 99 125 231 142 43 159 112 180 193 246 204 34 124 226 122 220 97 119 207 46 171 158 108 163 128 241 182 202 26 91 95 111 175 3\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:58 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-15998\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 106 155 95 110 170 156 99 128 244 193 245 197 6 10 26 92 99 125 230 139 32 114 188 228 132 259 254 234 156 98 121 216 84 68 257 247 206 44 163 126 234 155 93 103 142 44 161 120 211 63 237 166 140 36 130 251 222 108 162 122 219 96 116 195 254 236 163 127 239 175 173 167 142 44 163 126 236 163 126 235 159 110 172 164 131 255 237 167 141 37 135 14 43 158 106 155 95 111 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-15999\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 5 8 20 67 256 241 184 209 55 205 38 138 28 99 127 237 168 148 68 257 248 210 58 218 89 86 74 26 90 92 98 121 216 83 62 235 158 107 158 106 156 97 118 201 21 69 5 6 12 34 122 218 90 89 88 83 61 229 133 7 13 39 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   Writing example 0/15994\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-31989\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 34 123 222 106 154 92 97 118 204 34 121 213 72 18 59 221 102 140 33 118 201 24 82 57 215 77 38 139 30 106 154 89 85 70 12 33 119 205 40 146 58 218 90 91 93 102 140 33 118 204 33 118 204 35 126 234 154 90 92 100 131 254 235 157 101 135 15 47 173 166 137 24 81 55 206 41 149 70 11 31 109 168 147 64 241 182 204 33 117 197 8 18 58 217 87 77 40 147 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-31990\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 235 158 107 160 115 190 236 164 132 257 245 198 12 36 131 256 244 193 248 212 68 257 248 209 56 209 53 197 5 5 7 15 45 165 136 17 54 202 26 91 93 103 142 42 154 91 93 101 136 17 54 204 36 129 245 197 8 18 59 223 112 178 187 221 104 145 55 206 43 157 104 147 62 236 162 124 228 130 250 218 90 89 87 79 46 169 152 83 61 231 142 43 159 110 171 157 104 148 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-31991\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 175 175 173 166 138 26 91 95 110 171 157 104 148 65 245 198 9 21 69 5 8 18 60 227 125 232 147 61 232 146 60 227 127 238 172 163 126 236 162 124 228 132 260 257 247 205 40 147 62 236 161 120 212 68 259 253 232 146 60 225 120 212 67 255 239 174 172 164 132 260 257 248 211 62 236 163 126 236 163 125 232 148 67 253 232 147 61 232 148 66 252 228 132 259 256 244 196 257 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-31992\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 191 237 168 148 68 259 255 240 179 191 238 170 154 92 100 129 248 209 56 211 61 232 147 61 232 147 63 239 174 172 164 131 254 235 158 108 163 128 243 190 233 151 79 47 174 169 150 76 36 131 255 240 178 188 228 132 259 255 238 171 157 101 135 13 37 136 20 68 259 255 237 165 133 5 8 20 66 252 225 119 207 45 165 136 17 53 199 16 50 188 225 120 211 64 241 184 211 63 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   guid: train-31993\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   input_ids: 2 147 61 231 143 45 167 141 39 143 45 168 148 65 247 207 47 174 171 159 109 168 145 56 212 67 254 236 162 124 225 120 209 53 197 7 13 38 139 31 110 172 163 125 231 143 47 173 168 148 66 251 223 110 171 158 107 158 105 150 75 30 108 162 122 218 89 86 75 29 102 138 28 98 123 222 105 150 74 26 90 92 98 121 214 74 27 94 108 163 125 230 138 27 93 104 145 53 199 3\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:39:59 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   Writing example 0/15996\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   guid: train-47983\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   input_ids: 2 248 209 56 212 65 248 212 67 253 229 136 17 54 204 35 126 236 164 131 253 230 137 24 83 62 236 162 122 220 97 117 199 14 44 164 132 257 245 199 15 46 172 163 126 233 150 76 35 127 237 165 135 13 38 137 22 74 28 99 127 237 166 139 30 106 154 91 95 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   guid: train-47984\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   input_ids: 2 163 125 232 147 63 237 165 134 10 25 85 72 19 63 240 177 183 206 44 161 120 210 58 219 95 110 170 154 91 95 110 171 157 102 140 36 132 260 260 259 255 239 173 168 146 60 226 124 227 125 229 134 11 32 115 190 236 163 125 229 133 7 13 40 147 61 232 147 62 234 155 95 110 170 156 100 130 249 216 82 57 214 73 22 76 35 125 232 147 63 238 172 162 122 218 89 86 74 3\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   guid: train-47985\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   input_ids: 2 15 47 176 180 193 245 200 18 58 220 100 131 256 243 189 232 147 64 243 192 244 194 250 220 99 127 237 165 134 12 36 130 251 224 115 190 235 159 111 174 172 161 120 209 56 212 65 246 204 35 127 240 179 190 235 160 114 188 228 130 252 226 122 218 90 92 99 128 244 196 259 254 236 163 127 240 178 185 215 77 40 147 64 244 195 253 229 136 17 56 211 63 240 180 195 256 242 188 3\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   guid: train-47986\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   input_ids: 2 155 96 115 192 244 196 260 259 254 234 154 92 98 124 228 132 258 249 216 83 63 240 177 183 206 44 164 132 260 258 251 222 107 159 110 172 164 131 256 241 183 208 49 183 207 45 166 140 36 131 256 244 196 260 260 257 246 204 34 124 228 132 259 256 244 194 251 224 115 189 232 147 62 236 163 125 231 144 52 193 247 206 43 160 116 193 245 199 14 44 163 126 236 163 126 236 163 125 3\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   guid: train-47987\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   input_ids: 2 197 8 17 54 202 27 93 102 138 26 89 86 74 27 93 102 138 27 93 101 135 13 37 133 7 13 40 146 60 228 131 255 240 177 181 200 20 67 254 234 156 98 122 217 86 76 34 124 227 126 236 162 124 227 127 238 172 164 129 247 206 42 156 100 131 253 230 140 33 117 200 20 67 253 231 143 45 168 145 54 204 34 123 221 104 145 53 200 19 64 241 184 212 67 254 236 163 126 3\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 01:40:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 01:54:58 - INFO - transformers.data.processors.glue -   Writing example 10000/15994\n",
      "05/04/2023 01:54:59 - INFO - transformers.data.processors.glue -   Writing example 10000/15994\n",
      "05/04/2023 01:55:05 - INFO - transformers.data.processors.glue -   Writing example 10000/15996\n",
      "05/04/2023 01:56:06 - INFO - transformers.data.processors.glue -   Writing example 10000/15994\n",
      "05/04/2023 02:04:41 - INFO - __main__ -   Saving features into cached file ./data/kmer4/train/cached_train_4-new-12w-0_100_dnaprom\n",
      "05/04/2023 02:04:47 - INFO - __main__ -   ***** Running training *****\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Num examples = 63978\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Num Epochs = 3\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Instantaneous batch size per GPU = 128\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Total optimization steps = 1500\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Continuing training from epoch 0\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Continuing training from global step 0\n",
      "05/04/2023 02:04:47 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processor started !\n",
      "2 processor started !\n",
      "3 processor started !\n",
      "4 processor started !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A/home/ec2-user/SageMaker/DNABERT/src/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "Iteration:   0%|          | 1/500 [00:01<14:21,  1.73s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/500 [00:03<14:25,  1.74s/it]\u001b[A\n",
      "Iteration:   1%|          | 3/500 [00:05<14:25,  1.74s/it]\u001b[A\n",
      "Iteration:   1%|          | 4/500 [00:06<14:24,  1.74s/it]\u001b[A\n",
      "Iteration:   1%|          | 5/500 [00:08<14:23,  1.74s/it]\u001b[A\n",
      "Iteration:   1%|          | 6/500 [00:10<14:19,  1.74s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 7/500 [00:12<14:17,  1.74s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 8/500 [00:13<14:16,  1.74s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 9/500 [00:15<14:15,  1.74s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 10/500 [00:17<14:15,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 11/500 [00:19<14:14,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 12/500 [00:20<14:13,  1.75s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 13/500 [00:22<14:12,  1.75s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 14/500 [00:24<14:10,  1.75s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 15/500 [00:26<14:09,  1.75s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 16/500 [00:27<14:08,  1.75s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 17/500 [00:29<14:08,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 18/500 [00:31<14:07,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 19/500 [00:33<14:06,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 20/500 [00:35<14:06,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 21/500 [00:36<14:06,  1.77s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 22/500 [00:38<14:06,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 23/500 [00:40<14:06,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 24/500 [00:42<14:06,  1.78s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 25/500 [00:43<14:05,  1.78s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 26/500 [00:45<14:05,  1.78s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 27/500 [00:47<14:05,  1.79s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 28/500 [00:49<14:05,  1.79s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 29/500 [00:51<14:05,  1.80s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 30/500 [00:52<14:05,  1.80s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 31/500 [00:54<14:04,  1.80s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 32/500 [00:56<14:01,  1.80s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 33/500 [00:58<13:59,  1.80s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 34/500 [01:00<13:57,  1.80s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 35/500 [01:01<13:57,  1.80s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 36/500 [01:03<13:56,  1.80s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 37/500 [01:05<13:55,  1.80s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 38/500 [01:07<13:55,  1.81s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 39/500 [01:09<13:55,  1.81s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 40/500 [01:10<13:54,  1.82s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 41/500 [01:12<13:55,  1.82s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 42/500 [01:14<13:55,  1.82s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 43/500 [01:16<13:54,  1.83s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 44/500 [01:18<13:55,  1.83s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 45/500 [01:20<13:55,  1.84s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 46/500 [01:22<13:55,  1.84s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 47/500 [01:23<13:55,  1.84s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 48/500 [01:25<13:55,  1.85s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 49/500 [01:27<13:55,  1.85s/it]\u001b[A\n",
      "Iteration:  10%|█         | 50/500 [01:29<13:55,  1.86s/it]\u001b[A\n",
      "Iteration:  10%|█         | 51/500 [01:31<13:56,  1.86s/it]\u001b[A\n",
      "Iteration:  10%|█         | 52/500 [01:33<13:56,  1.87s/it]\u001b[A\n",
      "Iteration:  11%|█         | 53/500 [01:35<13:56,  1.87s/it]\u001b[A\n",
      "Iteration:  11%|█         | 54/500 [01:36<13:57,  1.88s/it]\u001b[A\n",
      "Iteration:  11%|█         | 55/500 [01:38<13:55,  1.88s/it]\u001b[A\n",
      "Iteration:  11%|█         | 56/500 [01:40<13:52,  1.87s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 57/500 [01:42<13:49,  1.87s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 58/500 [01:44<13:47,  1.87s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 59/500 [01:46<13:47,  1.88s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 60/500 [01:48<13:46,  1.88s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 61/500 [01:50<13:46,  1.88s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 62/500 [01:52<13:46,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 63/500 [01:53<13:46,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 64/500 [01:55<13:46,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 65/500 [01:57<13:46,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 66/500 [01:59<13:46,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 67/500 [02:01<13:47,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 68/500 [02:03<13:47,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 69/500 [02:05<13:47,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 70/500 [02:07<13:47,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 71/500 [02:09<13:48,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 72/500 [02:11<13:48,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 73/500 [02:13<13:49,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 74/500 [02:15<13:49,  1.95s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 75/500 [02:17<13:49,  1.95s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 76/500 [02:19<13:50,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 77/500 [02:21<13:50,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 78/500 [02:23<13:50,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 79/500 [02:25<13:50,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 80/500 [02:27<13:50,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 81/500 [02:29<13:50,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 82/500 [02:31<13:50,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 83/500 [02:33<13:49,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 84/500 [02:35<13:48,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 85/500 [02:37<13:48,  2.00s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 86/500 [02:39<13:47,  2.00s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 87/500 [02:41<13:46,  2.00s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 88/500 [02:43<13:43,  2.00s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 89/500 [02:45<13:40,  2.00s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 90/500 [02:47<13:37,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 91/500 [02:49<13:34,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 92/500 [02:51<13:31,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 93/500 [02:52<13:28,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 94/500 [02:54<13:26,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 95/500 [02:56<13:23,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 96/500 [02:58<13:23,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 97/500 [03:00<13:21,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 98/500 [03:02<13:18,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 99/500 [03:04<13:17,  1.99s/it]\u001b[A/home/ec2-user/anaconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  20%|██        | 100/500 [03:06<13:15,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██        | 101/500 [03:08<13:13,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██        | 102/500 [03:10<13:12,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 103/500 [03:12<13:11,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 104/500 [03:14<13:08,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 105/500 [03:16<13:07,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 106/500 [03:18<13:06,  2.00s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 107/500 [03:20<13:04,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 108/500 [03:22<13:02,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 109/500 [03:24<13:01,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 110/500 [03:26<12:58,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 111/500 [03:28<12:57,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 112/500 [03:30<12:55,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 113/500 [03:32<12:54,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 114/500 [03:34<12:53,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 115/500 [03:36<12:51,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 116/500 [03:38<12:48,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 117/500 [03:40<12:45,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 118/500 [03:42<12:43,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 119/500 [03:44<12:40,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 120/500 [03:46<12:37,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 121/500 [03:48<12:34,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 122/500 [03:50<12:33,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 123/500 [03:52<12:31,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 124/500 [03:54<12:29,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 125/500 [03:56<12:27,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 126/500 [03:58<12:26,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 127/500 [04:00<12:24,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 128/500 [04:02<12:22,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 129/500 [04:04<12:20,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 130/500 [04:06<12:18,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 131/500 [04:08<12:16,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 132/500 [04:10<12:14,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 133/500 [04:12<12:13,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 134/500 [04:14<12:11,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 135/500 [04:16<12:09,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 136/500 [04:18<12:07,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 137/500 [04:20<12:05,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 138/500 [04:22<12:03,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 139/500 [04:24<12:01,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 140/500 [04:26<11:59,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 141/500 [04:28<11:57,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 142/500 [04:30<11:56,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 143/500 [04:32<11:54,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 144/500 [04:34<11:51,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 145/500 [04:36<11:49,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 146/500 [04:38<11:47,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 147/500 [04:40<11:46,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 148/500 [04:42<11:44,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 149/500 [04:44<11:42,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|███       | 150/500 [04:46<11:41,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|███       | 151/500 [04:48<11:39,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|███       | 152/500 [04:50<11:37,  2.00s/it]\u001b[A\n",
      "Iteration:  31%|███       | 153/500 [04:52<11:35,  2.00s/it]\u001b[A\n",
      "Iteration:  31%|███       | 154/500 [04:54<11:32,  2.00s/it]\u001b[A\n",
      "Iteration:  31%|███       | 155/500 [04:56<11:26,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 156/500 [04:58<11:19,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 157/500 [05:00<11:10,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 158/500 [05:02<11:00,  1.93s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 159/500 [05:04<10:51,  1.91s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 160/500 [05:06<10:43,  1.89s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 161/500 [05:08<10:36,  1.88s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 162/500 [05:09<10:31,  1.87s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 163/500 [05:11<10:26,  1.86s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 164/500 [05:13<10:20,  1.85s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 165/500 [05:15<10:14,  1.84s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 166/500 [05:17<10:10,  1.83s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 167/500 [05:18<10:05,  1.82s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 168/500 [05:20<10:00,  1.81s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 169/500 [05:22<09:56,  1.80s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 170/500 [05:24<09:52,  1.80s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 171/500 [05:26<09:48,  1.79s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 172/500 [05:27<09:46,  1.79s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 173/500 [05:29<09:44,  1.79s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 174/500 [05:31<09:42,  1.79s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 175/500 [05:33<09:39,  1.78s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 176/500 [05:35<09:36,  1.78s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 177/500 [05:36<09:33,  1.78s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 178/500 [05:38<09:30,  1.77s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 179/500 [05:40<09:28,  1.77s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 180/500 [05:42<09:25,  1.77s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 181/500 [05:43<09:23,  1.76s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 182/500 [05:45<09:20,  1.76s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 183/500 [05:47<09:18,  1.76s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 184/500 [05:49<09:15,  1.76s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 185/500 [05:50<09:13,  1.76s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 186/500 [05:52<09:11,  1.76s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 187/500 [05:54<09:09,  1.75s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 188/500 [05:56<09:07,  1.76s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 189/500 [05:57<09:06,  1.76s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 190/500 [05:59<09:03,  1.75s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 191/500 [06:01<09:01,  1.75s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 192/500 [06:03<08:59,  1.75s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 193/500 [06:04<08:57,  1.75s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 194/500 [06:06<08:55,  1.75s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 195/500 [06:08<08:52,  1.75s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 196/500 [06:10<08:51,  1.75s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 197/500 [06:11<08:49,  1.75s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 198/500 [06:13<08:46,  1.74s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 199/500 [06:15<08:45,  1.74s/it]\u001b[A\n",
      "Iteration:  40%|████      | 200/500 [06:17<08:43,  1.74s/it]\u001b[A\n",
      "Iteration:  40%|████      | 201/500 [06:18<08:41,  1.74s/it]\u001b[A\n",
      "Iteration:  40%|████      | 202/500 [06:20<08:39,  1.74s/it]\u001b[A\n",
      "Iteration:  41%|████      | 203/500 [06:22<08:37,  1.74s/it]\u001b[A\n",
      "Iteration:  41%|████      | 204/500 [06:24<08:34,  1.74s/it]\u001b[A\n",
      "Iteration:  41%|████      | 205/500 [06:25<08:33,  1.74s/it]\u001b[A\n",
      "Iteration:  41%|████      | 206/500 [06:27<08:31,  1.74s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 207/500 [06:29<08:29,  1.74s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 208/500 [06:30<08:27,  1.74s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 209/500 [06:32<08:26,  1.74s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 210/500 [06:34<08:24,  1.74s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 211/500 [06:36<08:22,  1.74s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 212/500 [06:37<08:20,  1.74s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 213/500 [06:39<08:18,  1.74s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 214/500 [06:41<08:17,  1.74s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 215/500 [06:43<08:15,  1.74s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 216/500 [06:44<08:13,  1.74s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 217/500 [06:46<08:11,  1.74s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 218/500 [06:48<08:10,  1.74s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 219/500 [06:50<08:08,  1.74s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 220/500 [06:51<08:06,  1.74s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 221/500 [06:53<08:04,  1.74s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 222/500 [06:55<08:02,  1.74s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 223/500 [06:57<08:01,  1.74s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 224/500 [06:58<07:59,  1.74s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 225/500 [07:00<07:57,  1.74s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 226/500 [07:02<07:56,  1.74s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 227/500 [07:03<07:54,  1.74s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 228/500 [07:05<07:52,  1.74s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 229/500 [07:07<07:51,  1.74s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 230/500 [07:09<07:49,  1.74s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 231/500 [07:10<07:48,  1.74s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 232/500 [07:12<07:46,  1.74s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 233/500 [07:14<07:44,  1.74s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 234/500 [07:16<07:42,  1.74s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 235/500 [07:17<07:40,  1.74s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 236/500 [07:19<07:39,  1.74s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 237/500 [07:21<07:37,  1.74s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 238/500 [07:23<07:35,  1.74s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 239/500 [07:24<07:34,  1.74s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 240/500 [07:26<07:32,  1.74s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 241/500 [07:28<07:30,  1.74s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 242/500 [07:30<07:29,  1.74s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 243/500 [07:31<07:27,  1.74s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 244/500 [07:33<07:25,  1.74s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 245/500 [07:35<07:24,  1.74s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 246/500 [07:37<07:22,  1.74s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 247/500 [07:38<07:21,  1.74s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 248/500 [07:40<07:19,  1.75s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 249/500 [07:42<07:18,  1.75s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 250/500 [07:44<07:16,  1.74s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 251/500 [07:45<07:14,  1.75s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 252/500 [07:47<07:13,  1.75s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 253/500 [07:49<07:11,  1.75s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 254/500 [07:51<07:09,  1.75s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 255/500 [07:52<07:08,  1.75s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 256/500 [07:54<07:06,  1.75s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 257/500 [07:56<07:04,  1.75s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 258/500 [07:58<07:03,  1.75s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 259/500 [07:59<07:01,  1.75s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 260/500 [08:01<06:59,  1.75s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 261/500 [08:03<06:58,  1.75s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 262/500 [08:05<06:56,  1.75s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 263/500 [08:06<06:55,  1.75s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 264/500 [08:08<06:53,  1.75s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 265/500 [08:10<06:52,  1.75s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 266/500 [08:12<06:50,  1.76s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 267/500 [08:13<06:49,  1.76s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 268/500 [08:15<06:48,  1.76s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 269/500 [08:17<06:46,  1.76s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 270/500 [08:19<06:44,  1.76s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 271/500 [08:20<06:43,  1.76s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 272/500 [08:22<06:41,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 273/500 [08:24<06:39,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 274/500 [08:26<06:38,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 275/500 [08:27<06:36,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 276/500 [08:29<06:35,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 277/500 [08:31<06:33,  1.76s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 278/500 [08:33<06:32,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 279/500 [08:35<06:30,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 280/500 [08:36<06:28,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 281/500 [08:38<06:27,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 282/500 [08:40<06:25,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 283/500 [08:42<06:24,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 284/500 [08:43<06:22,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 285/500 [08:45<06:21,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 286/500 [08:47<06:20,  1.78s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 287/500 [08:49<06:18,  1.78s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 288/500 [08:50<06:17,  1.78s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 289/500 [08:52<06:15,  1.78s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 290/500 [08:54<06:13,  1.78s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 291/500 [08:56<06:12,  1.78s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 292/500 [08:58<06:11,  1.79s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 293/500 [08:59<06:10,  1.79s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 294/500 [09:01<06:08,  1.79s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 295/500 [09:03<06:07,  1.79s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 296/500 [09:05<06:05,  1.79s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 297/500 [09:07<06:04,  1.80s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 298/500 [09:08<06:03,  1.80s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 299/500 [09:10<06:01,  1.80s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 300/500 [09:12<06:00,  1.80s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 301/500 [09:14<05:58,  1.80s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 302/500 [09:16<05:56,  1.80s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 303/500 [09:17<05:54,  1.80s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 304/500 [09:19<05:52,  1.80s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 305/500 [09:21<05:50,  1.80s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 306/500 [09:23<05:48,  1.80s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 307/500 [09:25<05:47,  1.80s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 308/500 [09:26<05:45,  1.80s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 309/500 [09:28<05:44,  1.80s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 310/500 [09:30<05:43,  1.81s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 311/500 [09:32<05:41,  1.81s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 312/500 [09:34<05:40,  1.81s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 313/500 [09:36<05:39,  1.81s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 314/500 [09:37<05:38,  1.82s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 315/500 [09:39<05:37,  1.82s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 316/500 [09:41<05:35,  1.83s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 317/500 [09:43<05:35,  1.83s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 318/500 [09:45<05:33,  1.83s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 319/500 [09:47<05:32,  1.84s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 320/500 [09:48<05:31,  1.84s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 321/500 [09:50<05:30,  1.85s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 322/500 [09:52<05:29,  1.85s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 323/500 [09:54<05:28,  1.86s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 324/500 [09:56<05:27,  1.86s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 325/500 [09:58<05:26,  1.87s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 326/500 [10:00<05:25,  1.87s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 327/500 [10:02<05:24,  1.88s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 328/500 [10:03<05:22,  1.88s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 329/500 [10:05<05:20,  1.87s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 330/500 [10:07<05:18,  1.87s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 331/500 [10:09<05:16,  1.87s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 332/500 [10:11<05:15,  1.88s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 333/500 [10:13<05:13,  1.88s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 334/500 [10:15<05:12,  1.88s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 335/500 [10:17<05:10,  1.88s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 336/500 [10:18<05:09,  1.89s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 337/500 [10:20<05:08,  1.89s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 338/500 [10:22<05:07,  1.90s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 339/500 [10:24<05:06,  1.90s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 340/500 [10:26<05:05,  1.91s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 341/500 [10:28<05:03,  1.91s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 342/500 [10:30<05:02,  1.92s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 343/500 [10:32<05:01,  1.92s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 344/500 [10:34<05:00,  1.93s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 345/500 [10:36<04:59,  1.93s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 346/500 [10:38<04:58,  1.94s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 347/500 [10:40<04:57,  1.95s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 348/500 [10:42<04:56,  1.95s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 349/500 [10:44<04:55,  1.95s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 350/500 [10:46<04:53,  1.96s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 351/500 [10:48<04:52,  1.96s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 352/500 [10:50<04:51,  1.97s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 353/500 [10:52<04:50,  1.97s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 354/500 [10:53<04:48,  1.98s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 355/500 [10:55<04:47,  1.98s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 356/500 [10:57<04:45,  1.98s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 357/500 [10:59<04:44,  1.99s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 358/500 [11:01<04:43,  1.99s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 359/500 [11:03<04:41,  2.00s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 360/500 [11:05<04:39,  2.00s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 361/500 [11:07<04:37,  2.00s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 362/500 [11:09<04:35,  2.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 363/500 [11:11<04:33,  2.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 364/500 [11:13<04:31,  2.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 365/500 [11:15<04:29,  2.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 366/500 [11:17<04:27,  2.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 367/500 [11:19<04:25,  1.99s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 368/500 [11:21<04:23,  2.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 369/500 [11:23<04:21,  2.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 370/500 [11:25<04:19,  2.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 371/500 [11:27<04:17,  2.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 372/500 [11:29<04:15,  2.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 373/500 [11:31<04:13,  2.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 374/500 [11:33<04:11,  2.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 375/500 [11:35<04:09,  2.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 376/500 [11:37<04:08,  2.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 377/500 [11:39<04:06,  2.00s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 378/500 [11:41<04:04,  2.00s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 379/500 [11:43<04:02,  2.00s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 380/500 [11:45<03:59,  2.00s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 381/500 [11:47<03:57,  2.00s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 382/500 [11:49<03:55,  1.99s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 383/500 [11:51<03:53,  1.99s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 384/500 [11:53<03:50,  1.99s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 385/500 [11:55<03:48,  1.99s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 386/500 [11:57<03:47,  1.99s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 387/500 [11:59<03:44,  1.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 388/500 [12:01<03:43,  1.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 389/500 [12:03<03:40,  1.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 390/500 [12:05<03:38,  1.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 391/500 [12:07<03:36,  1.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 392/500 [12:09<03:34,  1.99s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 393/500 [12:11<03:32,  1.99s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 394/500 [12:13<03:31,  1.99s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 395/500 [12:15<03:29,  1.99s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 396/500 [12:17<03:27,  1.99s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 397/500 [12:19<03:25,  1.99s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 398/500 [12:21<03:23,  1.99s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 399/500 [12:23<03:21,  2.00s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 400/500 [12:25<03:19,  2.00s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 401/500 [12:27<03:17,  2.00s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 402/500 [12:29<03:15,  2.00s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 403/500 [12:31<03:13,  2.00s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 404/500 [12:33<03:11,  2.00s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 405/500 [12:35<03:09,  2.00s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 406/500 [12:37<03:07,  2.00s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 407/500 [12:39<03:05,  2.00s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 408/500 [12:41<03:03,  2.00s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 409/500 [12:43<03:01,  2.00s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 410/500 [12:45<02:59,  2.00s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 411/500 [12:47<02:57,  2.00s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 412/500 [12:49<02:55,  2.00s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 413/500 [12:51<02:53,  2.00s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 414/500 [12:53<02:51,  2.00s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 415/500 [12:55<02:49,  2.00s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 416/500 [12:57<02:48,  2.00s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 417/500 [12:59<02:46,  2.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 418/500 [13:01<02:44,  2.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 419/500 [13:03<02:42,  2.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 420/500 [13:05<02:40,  2.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 421/500 [13:07<02:38,  2.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 422/500 [13:09<02:36,  2.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 423/500 [13:11<02:34,  2.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 424/500 [13:13<02:32,  2.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 425/500 [13:15<02:30,  2.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 426/500 [13:17<02:28,  2.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 427/500 [13:19<02:26,  2.00s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 428/500 [13:21<02:24,  2.00s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 429/500 [13:23<02:22,  2.00s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 430/500 [13:25<02:20,  2.00s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 431/500 [13:27<02:18,  2.00s/it]\u001b[A\n",
      "Iteration:  86%|████████▋ | 432/500 [13:29<02:16,  2.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 433/500 [13:31<02:14,  2.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 434/500 [13:33<02:12,  2.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 435/500 [13:35<02:10,  2.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 436/500 [13:37<02:08,  2.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 437/500 [13:39<02:06,  2.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 438/500 [13:41<02:04,  2.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 439/500 [13:43<02:02,  2.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 440/500 [13:45<02:00,  2.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 441/500 [13:47<01:58,  2.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 442/500 [13:49<01:56,  2.01s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 443/500 [13:51<01:54,  2.01s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 444/500 [13:53<01:52,  2.01s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 445/500 [13:55<01:50,  2.00s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 446/500 [13:57<01:48,  2.00s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 447/500 [13:59<01:46,  2.00s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 448/500 [14:01<01:44,  2.00s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 449/500 [14:03<01:42,  2.00s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 450/500 [14:05<01:39,  2.00s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 451/500 [14:07<01:37,  1.99s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 452/500 [14:09<01:35,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 453/500 [14:11<01:33,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 454/500 [14:13<01:31,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 455/500 [14:15<01:29,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 456/500 [14:17<01:27,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 457/500 [14:19<01:25,  1.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 458/500 [14:21<01:23,  1.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 459/500 [14:23<01:21,  1.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 460/500 [14:25<01:19,  1.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 461/500 [14:27<01:17,  1.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 462/500 [14:29<01:15,  1.98s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 463/500 [14:31<01:13,  1.98s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 464/500 [14:33<01:11,  1.97s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 465/500 [14:35<01:09,  1.97s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 466/500 [14:37<01:07,  1.97s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 467/500 [14:39<01:05,  1.97s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 468/500 [14:41<01:03,  1.97s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 469/500 [14:43<01:01,  1.97s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 470/500 [14:45<00:59,  1.97s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 471/500 [14:47<00:57,  1.97s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 472/500 [14:49<00:55,  1.97s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 473/500 [14:51<00:53,  1.97s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 474/500 [14:53<00:51,  1.97s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 475/500 [14:55<00:49,  1.97s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 476/500 [14:57<00:47,  1.97s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 477/500 [14:59<00:45,  1.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 478/500 [15:01<00:43,  1.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 479/500 [15:03<00:41,  1.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 480/500 [15:05<00:39,  1.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 481/500 [15:07<00:37,  1.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████▋| 482/500 [15:09<00:35,  1.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 483/500 [15:11<00:33,  1.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 484/500 [15:12<00:31,  1.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 485/500 [15:14<00:29,  1.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 486/500 [15:16<00:27,  1.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 487/500 [15:18<00:25,  1.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 488/500 [15:20<00:23,  1.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 489/500 [15:22<00:21,  1.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 490/500 [15:24<00:19,  1.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 491/500 [15:26<00:17,  1.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 492/500 [15:28<00:15,  1.97s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 493/500 [15:30<00:13,  1.97s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 494/500 [15:32<00:11,  1.96s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 495/500 [15:34<00:09,  1.97s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 496/500 [15:36<00:07,  1.96s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 497/500 [15:38<00:05,  1.96s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 498/500 [15:40<00:03,  1.96s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 499/500 [15:42<00:01,  1.96s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 500/500 [15:44<00:00,  1.89s/it]\u001b[A\n",
      "05/04/2023 02:20:32 - INFO - __main__ -   Creating features from dataset file at ./data/kmer4/train/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.666666666666667e-06, \"loss\": 0.6584977096319199, \"step\": 100}\n",
      "{\"learning_rate\": 9.62962962962963e-06, \"loss\": 0.5799534258246422, \"step\": 200}\n",
      "{\"learning_rate\": 8.888888888888888e-06, \"loss\": 0.5456717628240585, \"step\": 300}\n",
      "{\"learning_rate\": 8.148148148148148e-06, \"loss\": 0.5275816050171852, \"step\": 400}\n",
      "{\"learning_rate\": 7.4074074074074075e-06, \"loss\": 0.5193141177296638, \"step\": 500}\n",
      "finish loading examples\n",
      "number of processes for converting feature: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 02:20:33 - INFO - transformers.data.processors.glue -   Writing example 0/6555\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   input_ids: 2 155 95 111 173 165 136 19 62 235 160 115 191 238 170 155 94 107 158 107 158 108 163 127 240 180 196 259 256 243 189 232 147 63 240 180 195 254 236 164 130 252 228 130 252 228 131 255 237 167 142 44 161 120 209 55 205 37 136 17 54 204 35 127 238 169 152 83 61 230 139 30 106 155 96 115 191 238 169 150 75 29 104 145 56 211 62 235 159 112 177 184 212 66 252 228 129 247 206 3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   input_ids: 2 152 81 55 208 51 191 240 178 185 215 80 50 188 227 127 237 168 145 54 204 36 130 252 226 122 217 87 79 46 172 164 129 248 211 62 234 153 85 69 5 5 8 19 62 236 163 125 231 144 51 189 229 136 18 60 226 122 217 85 69 7 14 42 155 94 108 162 121 216 84 65 248 212 65 245 198 9 23 78 44 162 122 220 100 131 256 242 186 219 93 101 133 5 8 19 61 230 138 3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   input_ids: 2 64 243 191 237 167 143 46 172 163 127 238 171 157 102 138 28 98 124 227 127 239 173 168 148 65 248 210 58 219 94 107 159 109 165 133 7 15 47 176 179 192 243 190 236 163 128 244 193 248 210 60 225 120 210 60 225 119 207 45 165 136 18 58 219 95 112 180 195 255 237 168 146 58 219 96 113 183 207 46 171 160 113 184 212 65 246 203 31 109 168 145 56 212 66 252 228 129 248 3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   input_ids: 2 158 108 163 128 243 192 242 185 213 70 12 36 131 253 232 147 64 243 191 240 178 188 228 131 255 238 171 160 115 192 242 187 223 109 166 139 30 106 154 92 99 127 240 178 186 219 94 107 158 107 160 116 193 247 207 46 172 162 123 221 103 141 37 133 8 20 65 248 210 59 224 115 192 243 191 240 179 191 240 179 191 240 179 191 240 179 191 239 175 175 174 171 159 111 174 171 159 112 3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   input_ids: 2 242 186 219 93 104 147 64 244 193 247 208 51 192 243 192 243 192 244 195 255 238 171 160 113 182 203 30 107 158 108 164 129 247 206 43 160 114 187 221 103 143 46 172 163 127 239 175 174 171 159 111 175 175 174 171 159 111 176 179 191 240 179 191 240 178 187 221 103 143 47 173 168 148 65 245 197 7 15 48 180 195 255 240 179 189 229 134 11 32 115 191 240 180 195 255 240 177 183 3\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 02:20:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 02:29:01 - INFO - __main__ -   Saving features into cached file ./data/kmer4/train/cached_dev_4-new-12w-0_100_dnaprom\n",
      "05/04/2023 02:29:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/04/2023 02:29:02 - INFO - __main__ -     Num examples = 6555\n",
      "05/04/2023 02:29:02 - INFO - __main__ -     Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processor started !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 1/52 [00:00<00:32,  1.57it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 2/52 [00:01<00:33,  1.50it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 3/52 [00:01<00:32,  1.49it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 4/52 [00:02<00:31,  1.51it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 5/52 [00:03<00:31,  1.49it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 6/52 [00:04<00:30,  1.49it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 7/52 [00:04<00:30,  1.49it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 8/52 [00:05<00:29,  1.48it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 9/52 [00:06<00:29,  1.48it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 10/52 [00:06<00:28,  1.48it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 11/52 [00:07<00:27,  1.48it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 12/52 [00:08<00:27,  1.48it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 13/52 [00:08<00:26,  1.48it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 14/52 [00:09<00:25,  1.47it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 15/52 [00:10<00:25,  1.47it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 16/52 [00:10<00:24,  1.47it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 17/52 [00:11<00:23,  1.47it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 18/52 [00:12<00:23,  1.47it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 19/52 [00:12<00:22,  1.48it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 20/52 [00:13<00:21,  1.48it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 21/52 [00:14<00:20,  1.48it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 22/52 [00:14<00:20,  1.48it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 23/52 [00:15<00:19,  1.48it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 24/52 [00:16<00:18,  1.48it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 25/52 [00:16<00:18,  1.47it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 26/52 [00:17<00:17,  1.46it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 27/52 [00:18<00:17,  1.46it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 28/52 [00:18<00:16,  1.48it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 29/52 [00:19<00:15,  1.47it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 30/52 [00:20<00:14,  1.47it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 31/52 [00:20<00:14,  1.47it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 32/52 [00:21<00:13,  1.47it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 33/52 [00:22<00:12,  1.47it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 34/52 [00:23<00:12,  1.47it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 35/52 [00:23<00:11,  1.47it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 36/52 [00:24<00:10,  1.46it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 37/52 [00:25<00:10,  1.46it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 38/52 [00:25<00:09,  1.46it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 39/52 [00:26<00:08,  1.46it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 40/52 [00:27<00:08,  1.46it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 41/52 [00:27<00:07,  1.46it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 42/52 [00:28<00:06,  1.46it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 43/52 [00:29<00:06,  1.46it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 44/52 [00:29<00:05,  1.46it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 45/52 [00:30<00:04,  1.45it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 46/52 [00:31<00:04,  1.45it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 47/52 [00:31<00:03,  1.45it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 48/52 [00:32<00:02,  1.45it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 49/52 [00:33<00:02,  1.45it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 50/52 [00:34<00:01,  1.45it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 51/52 [00:34<00:00,  1.44it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 52/52 [00:34<00:00,  1.49it/s]\u001b[A\n",
      "05/04/2023 02:29:37 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     acc = 0.7058733790999238\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     auc = 0.8017907617497524\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     f1 = 0.5606095457639112\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     mcc = 0.26488272067192997\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     precision = 0.5797408839822791\n",
      "05/04/2023 02:29:37 - INFO - __main__ -     recall = 0.7199714005119258\n",
      "Epoch:  33%|███▎      | 1/3 [24:49<49:38, 1489.15s/it]\n",
      "Iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 1/500 [00:01<16:14,  1.95s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/500 [00:03<16:09,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 3/500 [00:05<16:07,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 4/500 [00:07<16:08,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 5/500 [00:09<16:08,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 6/500 [00:11<16:08,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 7/500 [00:13<16:08,  1.97s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 8/500 [00:15<16:08,  1.97s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 9/500 [00:17<16:09,  1.97s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 10/500 [00:19<16:09,  1.98s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 11/500 [00:21<16:09,  1.98s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 12/500 [00:23<16:09,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 13/500 [00:25<16:09,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 14/500 [00:27<16:09,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 15/500 [00:29<16:09,  2.00s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 16/500 [00:31<16:08,  2.00s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 17/500 [00:33<16:07,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 18/500 [00:35<16:05,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 19/500 [00:37<16:04,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 20/500 [00:39<16:01,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 21/500 [00:41<15:59,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 22/500 [00:43<15:57,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 23/500 [00:45<15:54,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 24/500 [00:47<15:51,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 25/500 [00:49<15:50,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 26/500 [00:51<15:47,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 27/500 [00:53<15:45,  2.00s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 28/500 [00:55<15:43,  2.00s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 29/500 [00:57<15:41,  2.00s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 30/500 [00:59<15:38,  2.00s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 31/500 [01:01<15:37,  2.00s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 32/500 [01:03<15:35,  2.00s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 33/500 [01:05<15:32,  2.00s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 34/500 [01:07<15:31,  2.00s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 35/500 [01:09<15:30,  2.00s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 36/500 [01:11<15:28,  2.00s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 37/500 [01:13<15:26,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 38/500 [01:15<15:23,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 39/500 [01:17<15:21,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 40/500 [01:19<15:18,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 41/500 [01:21<15:16,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 42/500 [01:23<15:12,  1.99s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 43/500 [01:25<15:10,  1.99s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 44/500 [01:27<15:08,  1.99s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 45/500 [01:29<15:06,  1.99s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 46/500 [01:31<15:03,  1.99s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 47/500 [01:33<15:01,  1.99s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 48/500 [01:35<15:00,  1.99s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 49/500 [01:37<14:57,  1.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 50/500 [01:39<14:55,  1.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 51/500 [01:41<14:52,  1.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 52/500 [01:43<14:50,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 53/500 [01:45<14:48,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 54/500 [01:47<14:46,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 55/500 [01:49<14:44,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 56/500 [01:51<14:42,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 57/500 [01:53<14:41,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 58/500 [01:55<14:38,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 59/500 [01:57<14:36,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 60/500 [01:59<14:34,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 61/500 [02:01<14:33,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 62/500 [02:03<14:32,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 63/500 [02:05<14:29,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 64/500 [02:07<14:28,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 65/500 [02:09<14:26,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 66/500 [02:11<14:23,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 67/500 [02:13<14:22,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 68/500 [02:15<14:20,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 69/500 [02:17<14:18,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 70/500 [02:19<14:16,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 71/500 [02:21<14:14,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 72/500 [02:23<14:12,  1.99s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 73/500 [02:25<14:10,  1.99s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 74/500 [02:27<14:08,  1.99s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 75/500 [02:29<14:06,  1.99s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 76/500 [02:31<14:04,  1.99s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 77/500 [02:33<14:02,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 78/500 [02:35<14:00,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 79/500 [02:37<13:58,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 80/500 [02:39<13:56,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 81/500 [02:41<13:54,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 82/500 [02:43<13:52,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 83/500 [02:45<13:50,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 84/500 [02:47<13:48,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 85/500 [02:49<13:46,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 86/500 [02:51<13:44,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 87/500 [02:53<13:42,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 88/500 [02:55<13:39,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 89/500 [02:57<13:37,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 90/500 [02:59<13:35,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 91/500 [03:01<13:33,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 92/500 [03:03<13:31,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 93/500 [03:05<13:29,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 94/500 [03:07<13:28,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 95/500 [03:09<13:26,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 96/500 [03:11<13:24,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 97/500 [03:13<13:22,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 98/500 [03:15<13:20,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 99/500 [03:17<13:19,  1.99s/it]\u001b[A/home/ec2-user/anaconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  20%|██        | 100/500 [03:19<13:17,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██        | 101/500 [03:21<13:15,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██        | 102/500 [03:23<13:13,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 103/500 [03:25<13:11,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 104/500 [03:27<13:09,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 105/500 [03:29<13:06,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 106/500 [03:31<13:05,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 107/500 [03:33<13:02,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 108/500 [03:35<12:59,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 109/500 [03:37<12:58,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 110/500 [03:39<12:56,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 111/500 [03:40<12:54,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 112/500 [03:42<12:52,  1.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 113/500 [03:44<12:50,  1.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 114/500 [03:46<12:49,  1.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 115/500 [03:48<12:47,  1.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 116/500 [03:50<12:44,  1.99s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 117/500 [03:52<12:42,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 118/500 [03:54<12:40,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 119/500 [03:56<12:38,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 120/500 [03:58<12:35,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 121/500 [04:00<12:33,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 122/500 [04:02<12:31,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 123/500 [04:04<12:30,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 124/500 [04:06<12:27,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 125/500 [04:08<12:25,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 126/500 [04:10<12:23,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 127/500 [04:12<12:21,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 128/500 [04:14<12:19,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 129/500 [04:16<12:17,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 130/500 [04:18<12:16,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 131/500 [04:20<12:14,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 132/500 [04:22<12:12,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 133/500 [04:24<12:11,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 134/500 [04:26<12:08,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 135/500 [04:28<12:06,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 136/500 [04:30<12:05,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 137/500 [04:32<12:02,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 138/500 [04:34<12:00,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 139/500 [04:36<11:59,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 140/500 [04:38<11:57,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 141/500 [04:40<11:55,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 142/500 [04:42<11:53,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 143/500 [04:44<11:51,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 144/500 [04:46<11:49,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 145/500 [04:48<11:48,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 146/500 [04:50<11:45,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 147/500 [04:52<11:43,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 148/500 [04:54<11:41,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 149/500 [04:56<11:38,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 150/500 [04:58<11:36,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 151/500 [05:00<11:35,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 152/500 [05:02<11:32,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 153/500 [05:04<11:30,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 154/500 [05:06<11:28,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 155/500 [05:08<11:26,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 156/500 [05:10<11:24,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 157/500 [05:12<11:22,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 158/500 [05:14<11:20,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 159/500 [05:16<11:18,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 160/500 [05:18<11:16,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 161/500 [05:20<11:14,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 162/500 [05:22<11:12,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 163/500 [05:24<11:11,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 164/500 [05:26<11:09,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 165/500 [05:28<11:06,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 166/500 [05:30<11:04,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 167/500 [05:32<11:02,  1.99s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 168/500 [05:34<11:01,  1.99s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 169/500 [05:36<10:59,  1.99s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 170/500 [05:38<10:57,  1.99s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 171/500 [05:40<10:55,  1.99s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 172/500 [05:42<10:53,  1.99s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 173/500 [05:44<10:51,  1.99s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 174/500 [05:46<10:49,  1.99s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 175/500 [05:48<10:47,  1.99s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 176/500 [05:50<10:45,  1.99s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 177/500 [05:52<10:43,  1.99s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 178/500 [05:54<10:41,  1.99s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 179/500 [05:56<10:39,  1.99s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 180/500 [05:58<10:38,  1.99s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 181/500 [06:00<10:35,  1.99s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 182/500 [06:02<10:33,  1.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 183/500 [06:04<10:31,  1.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 184/500 [06:06<10:29,  1.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 185/500 [06:08<10:27,  1.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 186/500 [06:10<10:25,  1.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 187/500 [06:12<10:23,  1.99s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 188/500 [06:14<10:21,  1.99s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 189/500 [06:16<10:19,  1.99s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 190/500 [06:18<10:16,  1.99s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 191/500 [06:20<10:15,  1.99s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 192/500 [06:22<10:13,  1.99s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 193/500 [06:24<10:11,  1.99s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 194/500 [06:26<10:08,  1.99s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 195/500 [06:28<10:06,  1.99s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 196/500 [06:30<10:04,  1.99s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 197/500 [06:32<10:02,  1.99s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 198/500 [06:34<09:59,  1.99s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 199/500 [06:36<09:57,  1.99s/it]\u001b[A\n",
      "Iteration:  40%|████      | 200/500 [06:38<09:56,  1.99s/it]\u001b[A\n",
      "Iteration:  40%|████      | 201/500 [06:40<09:53,  1.99s/it]\u001b[A\n",
      "Iteration:  40%|████      | 202/500 [06:42<09:51,  1.99s/it]\u001b[A\n",
      "Iteration:  41%|████      | 203/500 [06:44<09:50,  1.99s/it]\u001b[A\n",
      "Iteration:  41%|████      | 204/500 [06:46<09:49,  1.99s/it]\u001b[A\n",
      "Iteration:  41%|████      | 205/500 [06:48<09:47,  1.99s/it]\u001b[A\n",
      "Iteration:  41%|████      | 206/500 [06:50<09:45,  1.99s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 207/500 [06:52<09:43,  1.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 208/500 [06:54<09:41,  1.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 209/500 [06:56<09:39,  1.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 210/500 [06:58<09:37,  1.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 211/500 [07:00<09:35,  1.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 212/500 [07:02<09:33,  1.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 213/500 [07:04<09:31,  1.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 214/500 [07:06<09:29,  1.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 215/500 [07:08<09:27,  1.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 216/500 [07:10<09:25,  1.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 217/500 [07:12<09:23,  1.99s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 218/500 [07:14<09:21,  1.99s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 219/500 [07:16<09:19,  1.99s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 220/500 [07:17<09:17,  1.99s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 221/500 [07:19<09:15,  1.99s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 222/500 [07:21<09:12,  1.99s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 223/500 [07:23<09:10,  1.99s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 224/500 [07:25<09:08,  1.99s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 225/500 [07:27<09:06,  1.99s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 226/500 [07:29<09:04,  1.99s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 227/500 [07:31<09:02,  1.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 228/500 [07:33<09:00,  1.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 229/500 [07:35<08:58,  1.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 230/500 [07:37<08:56,  1.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 231/500 [07:39<08:55,  1.99s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 232/500 [07:41<08:53,  1.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 233/500 [07:43<08:50,  1.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 234/500 [07:45<08:49,  1.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 235/500 [07:47<08:47,  1.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 236/500 [07:49<08:45,  1.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 237/500 [07:51<08:43,  1.99s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 238/500 [07:53<08:41,  1.99s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 239/500 [07:55<08:39,  1.99s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 240/500 [07:57<08:37,  1.99s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 241/500 [07:59<08:35,  1.99s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 242/500 [08:01<08:33,  1.99s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 243/500 [08:03<08:31,  1.99s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 244/500 [08:05<08:30,  1.99s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 245/500 [08:07<08:27,  1.99s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 246/500 [08:09<08:25,  1.99s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 247/500 [08:11<08:23,  1.99s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 248/500 [08:13<08:21,  1.99s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 249/500 [08:15<08:19,  1.99s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 250/500 [08:17<08:16,  1.99s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 251/500 [08:19<08:15,  1.99s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 252/500 [08:21<08:13,  1.99s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 253/500 [08:23<08:11,  1.99s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 254/500 [08:25<08:08,  1.99s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 255/500 [08:27<08:07,  1.99s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 256/500 [08:29<08:05,  1.99s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 257/500 [08:31<08:02,  1.98s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 258/500 [08:33<07:57,  1.97s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 259/500 [08:35<07:51,  1.95s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 260/500 [08:37<07:44,  1.93s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 261/500 [08:39<07:37,  1.92s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 262/500 [08:41<07:31,  1.90s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 263/500 [08:42<07:25,  1.88s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 264/500 [08:44<07:22,  1.87s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 265/500 [08:46<07:17,  1.86s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 266/500 [08:48<07:13,  1.85s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 267/500 [08:50<07:08,  1.84s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 268/500 [08:52<07:04,  1.83s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 269/500 [08:53<07:00,  1.82s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 270/500 [08:55<06:56,  1.81s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 271/500 [08:57<06:53,  1.81s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 272/500 [08:59<06:50,  1.80s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 273/500 [09:00<06:46,  1.79s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 274/500 [09:02<06:43,  1.79s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 275/500 [09:04<06:41,  1.78s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 276/500 [09:06<06:39,  1.78s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 277/500 [09:08<06:37,  1.78s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 278/500 [09:09<06:34,  1.78s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 279/500 [09:11<06:32,  1.78s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 280/500 [09:13<06:29,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 281/500 [09:15<06:27,  1.77s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 282/500 [09:16<06:25,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 283/500 [09:18<06:23,  1.77s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 284/500 [09:20<06:20,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 285/500 [09:22<06:18,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 286/500 [09:23<06:16,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 287/500 [09:25<06:14,  1.76s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 288/500 [09:27<06:11,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 289/500 [09:29<06:10,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 290/500 [09:30<06:07,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 291/500 [09:32<06:05,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 292/500 [09:34<06:04,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 293/500 [09:36<06:02,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 294/500 [09:37<06:00,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 295/500 [09:39<05:58,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 296/500 [09:41<05:56,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 297/500 [09:43<05:54,  1.75s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 298/500 [09:44<05:52,  1.75s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 299/500 [09:46<05:50,  1.74s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 300/500 [09:48<05:48,  1.74s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 301/500 [09:50<05:46,  1.74s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 302/500 [09:51<05:44,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 303/500 [09:53<05:43,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 304/500 [09:55<05:41,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 305/500 [09:57<05:39,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 306/500 [09:58<05:37,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 307/500 [10:00<05:35,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 308/500 [10:02<05:33,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 309/500 [10:04<05:32,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 310/500 [10:05<05:30,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 311/500 [10:07<05:28,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 312/500 [10:09<05:26,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 313/500 [10:11<05:24,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 314/500 [10:12<05:23,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 315/500 [10:14<05:21,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 316/500 [10:16<05:19,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 317/500 [10:17<05:18,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 318/500 [10:19<05:16,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 319/500 [10:21<05:14,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 320/500 [10:23<05:12,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 321/500 [10:24<05:10,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 322/500 [10:26<05:09,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 323/500 [10:28<05:07,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 324/500 [10:30<05:05,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 325/500 [10:31<05:03,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 326/500 [10:33<05:02,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 327/500 [10:35<05:00,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 328/500 [10:37<04:58,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 329/500 [10:38<04:57,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 330/500 [10:40<04:55,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 331/500 [10:42<04:53,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 332/500 [10:44<04:51,  1.74s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 333/500 [10:45<04:50,  1.74s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 334/500 [10:47<04:48,  1.74s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 335/500 [10:49<04:46,  1.74s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 336/500 [10:50<04:45,  1.74s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 337/500 [10:52<04:43,  1.74s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 338/500 [10:54<04:41,  1.74s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 339/500 [10:56<04:40,  1.74s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 340/500 [10:57<04:38,  1.74s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 341/500 [10:59<04:36,  1.74s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 342/500 [11:01<04:34,  1.74s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 343/500 [11:03<04:33,  1.74s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 344/500 [11:04<04:31,  1.74s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 345/500 [11:06<04:29,  1.74s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 346/500 [11:08<04:28,  1.74s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 347/500 [11:10<04:26,  1.74s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 348/500 [11:11<04:25,  1.74s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 349/500 [11:13<04:23,  1.74s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 350/500 [11:15<04:21,  1.74s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 351/500 [11:17<04:19,  1.74s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 352/500 [11:18<04:18,  1.75s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 353/500 [11:20<04:16,  1.75s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 354/500 [11:22<04:14,  1.75s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 355/500 [11:24<04:13,  1.75s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 356/500 [11:25<04:11,  1.75s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 357/500 [11:27<04:10,  1.75s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 358/500 [11:29<04:08,  1.75s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 359/500 [11:31<04:06,  1.75s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 360/500 [11:32<04:05,  1.75s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 361/500 [11:34<04:03,  1.75s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 362/500 [11:36<04:01,  1.75s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 363/500 [11:38<03:59,  1.75s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 364/500 [11:39<03:58,  1.75s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 365/500 [11:41<03:56,  1.75s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 366/500 [11:43<03:54,  1.75s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 367/500 [11:45<03:52,  1.75s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 368/500 [11:46<03:51,  1.75s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 369/500 [11:48<03:49,  1.75s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 370/500 [11:50<03:48,  1.75s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 371/500 [11:52<03:46,  1.76s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 372/500 [11:53<03:44,  1.76s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 373/500 [11:55<03:43,  1.76s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 374/500 [11:57<03:41,  1.76s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 375/500 [11:59<03:39,  1.76s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 376/500 [12:00<03:38,  1.76s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 377/500 [12:02<03:36,  1.76s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 378/500 [12:04<03:34,  1.76s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 379/500 [12:06<03:33,  1.76s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 380/500 [12:07<03:31,  1.76s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 381/500 [12:09<03:29,  1.76s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 382/500 [12:11<03:28,  1.77s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 383/500 [12:13<03:26,  1.77s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 384/500 [12:15<03:25,  1.77s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 385/500 [12:16<03:23,  1.77s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 386/500 [12:18<03:21,  1.77s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 387/500 [12:20<03:20,  1.78s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 388/500 [12:22<03:18,  1.78s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 389/500 [12:23<03:17,  1.78s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 390/500 [12:25<03:15,  1.78s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 391/500 [12:27<03:14,  1.78s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 392/500 [12:29<03:12,  1.78s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 393/500 [12:31<03:10,  1.78s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 394/500 [12:32<03:08,  1.78s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 395/500 [12:34<03:07,  1.78s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 396/500 [12:36<03:05,  1.79s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 397/500 [12:38<03:04,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 398/500 [12:40<03:02,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 399/500 [12:41<03:01,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 400/500 [12:43<02:59,  1.80s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 401/500 [12:45<02:57,  1.80s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 402/500 [12:47<02:56,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 403/500 [12:49<02:54,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 404/500 [12:50<02:53,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 405/500 [12:52<02:50,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 406/500 [12:54<02:49,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 407/500 [12:56<02:47,  1.80s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 408/500 [12:58<02:45,  1.80s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 409/500 [12:59<02:43,  1.80s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 410/500 [13:01<02:41,  1.80s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 411/500 [13:03<02:40,  1.80s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 412/500 [13:05<02:38,  1.80s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 413/500 [13:07<02:36,  1.80s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 414/500 [13:08<02:34,  1.80s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 415/500 [13:10<02:32,  1.79s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 416/500 [13:12<02:30,  1.79s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 417/500 [13:14<02:27,  1.78s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 418/500 [13:15<02:25,  1.78s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 419/500 [13:17<02:23,  1.78s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 420/500 [13:19<02:22,  1.78s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 421/500 [13:21<02:20,  1.77s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 422/500 [13:23<02:18,  1.77s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 423/500 [13:24<02:16,  1.77s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 424/500 [13:26<02:14,  1.76s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 425/500 [13:28<02:12,  1.76s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 426/500 [13:30<02:10,  1.76s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 427/500 [13:31<02:08,  1.76s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 428/500 [13:33<02:06,  1.76s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 429/500 [13:35<02:04,  1.75s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 430/500 [13:37<02:02,  1.75s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 431/500 [13:38<02:00,  1.75s/it]\u001b[A\n",
      "Iteration:  86%|████████▋ | 432/500 [13:40<01:58,  1.75s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 433/500 [13:42<01:56,  1.75s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 434/500 [13:44<01:55,  1.74s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 435/500 [13:45<01:53,  1.74s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 436/500 [13:47<01:51,  1.74s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 437/500 [13:49<01:49,  1.74s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 438/500 [13:50<01:47,  1.74s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 439/500 [13:52<01:46,  1.74s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 440/500 [13:54<01:44,  1.74s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 441/500 [13:56<01:42,  1.74s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 442/500 [13:57<01:40,  1.74s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 443/500 [13:59<01:38,  1.74s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 444/500 [14:01<01:37,  1.73s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 445/500 [14:03<01:35,  1.73s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 446/500 [14:04<01:33,  1.74s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 447/500 [14:06<01:31,  1.74s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 448/500 [14:08<01:30,  1.74s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 449/500 [14:10<01:28,  1.74s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 450/500 [14:11<01:27,  1.74s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 451/500 [14:13<01:25,  1.74s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 452/500 [14:15<01:23,  1.74s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 453/500 [14:17<01:21,  1.74s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 454/500 [14:18<01:19,  1.74s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 455/500 [14:20<01:18,  1.74s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 456/500 [14:22<01:16,  1.73s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 457/500 [14:23<01:14,  1.73s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 458/500 [14:25<01:12,  1.73s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 459/500 [14:27<01:11,  1.73s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 460/500 [14:29<01:09,  1.73s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 461/500 [14:30<01:07,  1.73s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 462/500 [14:32<01:05,  1.73s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 463/500 [14:34<01:04,  1.73s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 464/500 [14:36<01:02,  1.73s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 465/500 [14:37<01:00,  1.73s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 466/500 [14:39<00:58,  1.73s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 467/500 [14:41<00:57,  1.73s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 468/500 [14:43<00:55,  1.73s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 469/500 [14:44<00:53,  1.73s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 470/500 [14:46<00:52,  1.73s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 471/500 [14:48<00:50,  1.73s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 472/500 [14:49<00:48,  1.73s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 473/500 [14:51<00:46,  1.73s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 474/500 [14:53<00:45,  1.73s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 475/500 [14:55<00:43,  1.73s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 476/500 [14:56<00:41,  1.73s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 477/500 [14:58<00:39,  1.73s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 478/500 [15:00<00:38,  1.73s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 479/500 [15:02<00:36,  1.73s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 480/500 [15:03<00:34,  1.73s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 481/500 [15:05<00:32,  1.73s/it]\u001b[A\n",
      "Iteration:  96%|█████████▋| 482/500 [15:07<00:31,  1.73s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 483/500 [15:09<00:29,  1.73s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 484/500 [15:10<00:27,  1.73s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 485/500 [15:12<00:26,  1.73s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 486/500 [15:14<00:24,  1.74s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 487/500 [15:15<00:22,  1.74s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 488/500 [15:17<00:20,  1.74s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 489/500 [15:19<00:19,  1.74s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 490/500 [15:21<00:17,  1.74s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 491/500 [15:22<00:15,  1.74s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 492/500 [15:24<00:13,  1.74s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 493/500 [15:26<00:12,  1.74s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 494/500 [15:28<00:10,  1.74s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 495/500 [15:29<00:08,  1.74s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 496/500 [15:31<00:06,  1.74s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 497/500 [15:33<00:05,  1.74s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 498/500 [15:35<00:03,  1.74s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 499/500 [15:36<00:01,  1.74s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 500/500 [15:38<00:00,  1.88s/it]\u001b[A\n",
      "05/04/2023 02:45:15 - INFO - __main__ -   Loading features from cached file ./data/kmer4/train/cached_dev_4-new-12w-0_100_dnaprom\n",
      "05/04/2023 02:45:15 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/04/2023 02:45:15 - INFO - __main__ -     Num examples = 6555\n",
      "05/04/2023 02:45:15 - INFO - __main__ -     Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 6.666666666666667e-06, \"loss\": 0.5020298945903778, \"step\": 600}\n",
      "{\"learning_rate\": 5.925925925925926e-06, \"loss\": 0.5098447868227959, \"step\": 700}\n",
      "{\"learning_rate\": 5.185185185185185e-06, \"loss\": 0.5079209694266319, \"step\": 800}\n",
      "{\"learning_rate\": 4.444444444444444e-06, \"loss\": 0.4917222657799721, \"step\": 900}\n",
      "{\"learning_rate\": 3.7037037037037037e-06, \"loss\": 0.49487869441509247, \"step\": 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 1/52 [00:00<00:30,  1.66it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 2/52 [00:01<00:31,  1.61it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 3/52 [00:01<00:30,  1.61it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 4/52 [00:02<00:29,  1.61it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 5/52 [00:03<00:29,  1.61it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 6/52 [00:03<00:28,  1.62it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 7/52 [00:04<00:27,  1.61it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 8/52 [00:04<00:27,  1.61it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 9/52 [00:05<00:26,  1.62it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 10/52 [00:06<00:26,  1.61it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 11/52 [00:06<00:25,  1.61it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 12/52 [00:07<00:24,  1.61it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 13/52 [00:08<00:24,  1.61it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 14/52 [00:08<00:23,  1.61it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 15/52 [00:09<00:22,  1.61it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 16/52 [00:09<00:22,  1.61it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 17/52 [00:10<00:21,  1.61it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 18/52 [00:11<00:21,  1.61it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 19/52 [00:11<00:20,  1.61it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 20/52 [00:12<00:19,  1.61it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 21/52 [00:13<00:19,  1.61it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 22/52 [00:13<00:18,  1.61it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 23/52 [00:14<00:18,  1.61it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 24/52 [00:14<00:17,  1.61it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 25/52 [00:15<00:16,  1.61it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 26/52 [00:16<00:16,  1.61it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 27/52 [00:16<00:15,  1.61it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 28/52 [00:17<00:14,  1.61it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 29/52 [00:18<00:14,  1.61it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 30/52 [00:18<00:13,  1.61it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 31/52 [00:19<00:13,  1.61it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 32/52 [00:19<00:12,  1.61it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 33/52 [00:20<00:11,  1.60it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 34/52 [00:21<00:11,  1.60it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 35/52 [00:21<00:10,  1.61it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 36/52 [00:22<00:09,  1.60it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 37/52 [00:22<00:09,  1.60it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 38/52 [00:23<00:08,  1.60it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 39/52 [00:24<00:08,  1.60it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 40/52 [00:24<00:07,  1.60it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 41/52 [00:25<00:06,  1.60it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 42/52 [00:26<00:06,  1.60it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 43/52 [00:26<00:05,  1.60it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 44/52 [00:27<00:05,  1.60it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 45/52 [00:27<00:04,  1.60it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 46/52 [00:28<00:03,  1.61it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 47/52 [00:29<00:03,  1.61it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 48/52 [00:29<00:02,  1.62it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 49/52 [00:30<00:01,  1.62it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 50/52 [00:31<00:01,  1.62it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 51/52 [00:31<00:00,  1.62it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 52/52 [00:31<00:00,  1.63it/s]\u001b[A\n",
      "05/04/2023 02:45:47 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     acc = 0.811441647597254\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     auc = 0.8092651180023738\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     f1 = 0.6323531814097851\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     mcc = 0.32456106832919307\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     precision = 0.6126865354894926\n",
      "05/04/2023 02:45:47 - INFO - __main__ -     recall = 0.733701139664573\n",
      "Epoch:  67%|██████▋   | 2/3 [40:59<19:44, 1184.12s/it]\n",
      "Iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 1/500 [00:01<14:33,  1.75s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/500 [00:03<14:31,  1.75s/it]\u001b[A\n",
      "Iteration:   1%|          | 3/500 [00:05<14:30,  1.75s/it]\u001b[A\n",
      "Iteration:   1%|          | 4/500 [00:07<14:28,  1.75s/it]\u001b[A\n",
      "Iteration:   1%|          | 5/500 [00:08<14:26,  1.75s/it]\u001b[A\n",
      "Iteration:   1%|          | 6/500 [00:10<14:25,  1.75s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 7/500 [00:12<14:23,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 8/500 [00:14<14:21,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 9/500 [00:15<14:20,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 10/500 [00:17<14:19,  1.75s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 11/500 [00:19<14:18,  1.76s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 12/500 [00:21<14:17,  1.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 13/500 [00:22<14:15,  1.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 14/500 [00:24<14:14,  1.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 15/500 [00:26<14:13,  1.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 16/500 [00:28<14:12,  1.76s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 17/500 [00:29<14:10,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 18/500 [00:31<14:09,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 19/500 [00:33<14:07,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 20/500 [00:35<14:06,  1.76s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 21/500 [00:36<14:05,  1.77s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 22/500 [00:38<14:04,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 23/500 [00:40<14:02,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 24/500 [00:42<14:01,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 25/500 [00:43<14:00,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 26/500 [00:45<13:59,  1.77s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 27/500 [00:47<13:58,  1.77s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 28/500 [00:49<13:56,  1.77s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 29/500 [00:51<13:56,  1.78s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 30/500 [00:52<13:55,  1.78s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 31/500 [00:54<13:54,  1.78s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 32/500 [00:56<13:53,  1.78s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 33/500 [00:58<13:53,  1.78s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 34/500 [01:00<13:51,  1.79s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 35/500 [01:01<13:51,  1.79s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 36/500 [01:03<13:50,  1.79s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 37/500 [01:05<13:49,  1.79s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 38/500 [01:07<13:48,  1.79s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 39/500 [01:09<13:47,  1.79s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 40/500 [01:10<13:46,  1.80s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 41/500 [01:12<13:45,  1.80s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 42/500 [01:14<13:42,  1.80s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 43/500 [01:16<13:39,  1.79s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 44/500 [01:17<13:37,  1.79s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 45/500 [01:19<13:35,  1.79s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 46/500 [01:21<13:34,  1.79s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 47/500 [01:23<13:33,  1.80s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 48/500 [01:25<13:32,  1.80s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 49/500 [01:26<13:31,  1.80s/it]\u001b[A\n",
      "Iteration:  10%|█         | 50/500 [01:28<13:30,  1.80s/it]\u001b[A\n",
      "Iteration:  10%|█         | 51/500 [01:30<13:30,  1.80s/it]\u001b[A\n",
      "Iteration:  10%|█         | 52/500 [01:32<13:29,  1.81s/it]\u001b[A\n",
      "Iteration:  11%|█         | 53/500 [01:34<13:29,  1.81s/it]\u001b[A\n",
      "Iteration:  11%|█         | 54/500 [01:36<13:28,  1.81s/it]\u001b[A\n",
      "Iteration:  11%|█         | 55/500 [01:37<13:28,  1.82s/it]\u001b[A\n",
      "Iteration:  11%|█         | 56/500 [01:39<13:27,  1.82s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 57/500 [01:41<13:27,  1.82s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 58/500 [01:43<13:27,  1.83s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 59/500 [01:45<13:28,  1.83s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 60/500 [01:47<13:28,  1.84s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 61/500 [01:48<13:28,  1.84s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 62/500 [01:50<13:28,  1.84s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 63/500 [01:52<13:28,  1.85s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 64/500 [01:54<13:28,  1.85s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 65/500 [01:56<13:28,  1.86s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 66/500 [01:58<13:29,  1.86s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 67/500 [02:00<13:29,  1.87s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 68/500 [02:01<13:25,  1.86s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 69/500 [02:03<13:23,  1.86s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 70/500 [02:05<13:21,  1.86s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 71/500 [02:07<13:20,  1.87s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 72/500 [02:09<13:19,  1.87s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 73/500 [02:11<13:18,  1.87s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 74/500 [02:13<13:18,  1.87s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 75/500 [02:15<13:18,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 76/500 [02:16<13:18,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 77/500 [02:18<13:18,  1.89s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 78/500 [02:20<13:18,  1.89s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 79/500 [02:22<13:18,  1.90s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 80/500 [02:24<13:18,  1.90s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 81/500 [02:26<13:18,  1.90s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 82/500 [02:28<13:18,  1.91s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 83/500 [02:30<13:19,  1.92s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 84/500 [02:32<13:19,  1.92s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 85/500 [02:34<13:20,  1.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 86/500 [02:36<13:20,  1.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 87/500 [02:38<13:21,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 88/500 [02:40<13:21,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 89/500 [02:42<13:20,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 90/500 [02:44<13:20,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 91/500 [02:45<13:20,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 92/500 [02:47<13:20,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 93/500 [02:49<13:20,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 94/500 [02:51<13:21,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 95/500 [02:53<13:20,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 96/500 [02:55<13:20,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 97/500 [02:57<13:19,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 98/500 [02:59<13:19,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 99/500 [03:01<13:19,  1.99s/it]\u001b[A/home/ec2-user/anaconda3/envs/dnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  20%|██        | 100/500 [03:03<13:17,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|██        | 101/500 [03:05<13:16,  2.00s/it]\u001b[A\n",
      "Iteration:  20%|██        | 102/500 [03:07<13:15,  2.00s/it]\u001b[A\n",
      "Iteration:  21%|██        | 103/500 [03:09<13:12,  2.00s/it]\u001b[A\n",
      "Iteration:  21%|██        | 104/500 [03:11<13:10,  2.00s/it]\u001b[A\n",
      "Iteration:  21%|██        | 105/500 [03:13<13:07,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 106/500 [03:15<13:05,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 107/500 [03:17<13:03,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 108/500 [03:19<13:01,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 109/500 [03:21<12:59,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 110/500 [03:23<12:58,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 111/500 [03:25<12:55,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 112/500 [03:27<12:55,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 113/500 [03:29<12:53,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 114/500 [03:31<12:50,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 115/500 [03:33<12:49,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 116/500 [03:35<12:48,  2.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 117/500 [03:37<12:46,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 118/500 [03:39<12:44,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 119/500 [03:41<12:42,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 120/500 [03:43<12:41,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 121/500 [03:45<12:38,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 122/500 [03:47<12:35,  2.00s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 123/500 [03:49<12:33,  2.00s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 124/500 [03:51<12:30,  2.00s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 125/500 [03:53<12:28,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 126/500 [03:55<12:25,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 127/500 [03:57<12:23,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 128/500 [03:59<12:20,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 129/500 [04:01<12:17,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 130/500 [04:03<12:16,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 131/500 [04:05<12:14,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 132/500 [04:07<12:13,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 133/500 [04:09<12:11,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 134/500 [04:11<12:10,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 135/500 [04:13<12:09,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 136/500 [04:15<12:06,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 137/500 [04:17<12:05,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 138/500 [04:19<12:04,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 139/500 [04:21<12:02,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 140/500 [04:23<12:00,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 141/500 [04:25<11:59,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 142/500 [04:27<11:57,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 143/500 [04:29<11:55,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 144/500 [04:31<11:53,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 145/500 [04:33<11:50,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 146/500 [04:35<11:47,  2.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 147/500 [04:37<11:45,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 148/500 [04:39<11:42,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 149/500 [04:41<11:39,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 150/500 [04:43<11:36,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 151/500 [04:45<11:34,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 152/500 [04:47<11:31,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 153/500 [04:49<11:29,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 154/500 [04:51<11:27,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 155/500 [04:53<11:24,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 156/500 [04:55<11:22,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 157/500 [04:57<11:20,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 158/500 [04:59<11:18,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 159/500 [05:01<11:15,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 160/500 [05:03<11:13,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 161/500 [05:05<11:12,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 162/500 [05:07<11:09,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 163/500 [05:09<11:07,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 164/500 [05:11<11:05,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 165/500 [05:13<11:02,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 166/500 [05:15<11:00,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 167/500 [05:17<10:58,  1.98s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 168/500 [05:19<10:56,  1.98s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 169/500 [05:21<10:54,  1.98s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 170/500 [05:23<10:52,  1.98s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 171/500 [05:25<10:50,  1.98s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 172/500 [05:27<10:48,  1.98s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 173/500 [05:29<10:46,  1.98s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 174/500 [05:31<10:43,  1.98s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 175/500 [05:33<10:41,  1.97s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 176/500 [05:35<10:39,  1.97s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 177/500 [05:37<10:37,  1.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 178/500 [05:39<10:35,  1.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 179/500 [05:41<10:33,  1.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 180/500 [05:43<10:31,  1.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 181/500 [05:45<10:29,  1.97s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 182/500 [05:47<10:27,  1.97s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 183/500 [05:49<10:25,  1.97s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 184/500 [05:50<10:23,  1.97s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 185/500 [05:52<10:22,  1.98s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 186/500 [05:54<10:20,  1.98s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 187/500 [05:56<10:18,  1.98s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 188/500 [05:58<10:16,  1.97s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 189/500 [06:00<10:14,  1.97s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 190/500 [06:02<10:12,  1.97s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 191/500 [06:04<10:10,  1.98s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 192/500 [06:06<10:08,  1.98s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 193/500 [06:08<10:06,  1.98s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 194/500 [06:10<10:04,  1.98s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 195/500 [06:12<10:02,  1.97s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 196/500 [06:14<10:00,  1.97s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 197/500 [06:16<09:58,  1.97s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 198/500 [06:18<09:56,  1.97s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 199/500 [06:20<09:54,  1.97s/it]\u001b[A\n",
      "Iteration:  40%|████      | 200/500 [06:22<09:52,  1.97s/it]\u001b[A\n",
      "Iteration:  40%|████      | 201/500 [06:24<09:50,  1.97s/it]\u001b[A\n",
      "Iteration:  40%|████      | 202/500 [06:26<09:48,  1.97s/it]\u001b[A\n",
      "Iteration:  41%|████      | 203/500 [06:28<09:46,  1.97s/it]\u001b[A\n",
      "Iteration:  41%|████      | 204/500 [06:30<09:43,  1.97s/it]\u001b[A\n",
      "Iteration:  41%|████      | 205/500 [06:32<09:41,  1.97s/it]\u001b[A\n",
      "Iteration:  41%|████      | 206/500 [06:34<09:39,  1.97s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 207/500 [06:36<09:37,  1.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 208/500 [06:38<09:36,  1.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 209/500 [06:40<09:33,  1.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 210/500 [06:42<09:31,  1.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 211/500 [06:44<09:29,  1.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 212/500 [06:46<09:27,  1.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 213/500 [06:48<09:25,  1.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 214/500 [06:50<09:23,  1.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 215/500 [06:52<09:21,  1.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 216/500 [06:54<09:19,  1.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 217/500 [06:56<09:17,  1.97s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 218/500 [06:58<09:16,  1.97s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 219/500 [07:00<09:13,  1.97s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 220/500 [07:02<09:11,  1.97s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 221/500 [07:03<09:09,  1.97s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 222/500 [07:05<09:07,  1.97s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 223/500 [07:07<09:05,  1.97s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 224/500 [07:09<09:03,  1.97s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 225/500 [07:11<09:01,  1.97s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 226/500 [07:13<08:59,  1.97s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 227/500 [07:15<08:57,  1.97s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 228/500 [07:17<08:55,  1.97s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 229/500 [07:19<08:53,  1.97s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 230/500 [07:21<08:51,  1.97s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 231/500 [07:23<08:49,  1.97s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 232/500 [07:25<08:47,  1.97s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 233/500 [07:27<08:45,  1.97s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 234/500 [07:29<08:46,  1.98s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 235/500 [07:31<08:43,  1.97s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 236/500 [07:33<08:40,  1.97s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 237/500 [07:35<08:38,  1.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 238/500 [07:37<08:36,  1.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 239/500 [07:39<08:34,  1.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 240/500 [07:41<08:32,  1.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 241/500 [07:43<08:30,  1.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 242/500 [07:45<08:28,  1.97s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 243/500 [07:47<08:26,  1.97s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 244/500 [07:49<08:24,  1.97s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 245/500 [07:51<08:22,  1.97s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 246/500 [07:53<08:20,  1.97s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 247/500 [07:55<08:18,  1.97s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 248/500 [07:57<08:16,  1.97s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 249/500 [07:59<08:14,  1.97s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 250/500 [08:01<08:10,  1.96s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 251/500 [08:03<08:04,  1.95s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 252/500 [08:04<07:58,  1.93s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 253/500 [08:06<07:52,  1.91s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 254/500 [08:08<07:45,  1.89s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 255/500 [08:10<07:39,  1.88s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 256/500 [08:12<07:34,  1.86s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 257/500 [08:14<07:30,  1.85s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 258/500 [08:15<07:26,  1.84s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 259/500 [08:17<07:22,  1.83s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 260/500 [08:19<07:18,  1.83s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 261/500 [08:21<07:15,  1.82s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 262/500 [08:23<07:11,  1.81s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 263/500 [08:24<07:07,  1.81s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 264/500 [08:26<07:04,  1.80s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 265/500 [08:28<07:01,  1.79s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 266/500 [08:30<06:57,  1.79s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 267/500 [08:32<06:54,  1.78s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 268/500 [08:33<06:52,  1.78s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 269/500 [08:35<06:50,  1.78s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 270/500 [08:37<06:48,  1.78s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 271/500 [08:39<06:46,  1.77s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 272/500 [08:40<06:43,  1.77s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 273/500 [08:42<06:41,  1.77s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 274/500 [08:44<06:39,  1.77s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 275/500 [08:46<06:36,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 276/500 [08:47<06:34,  1.76s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 277/500 [08:49<06:32,  1.76s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 278/500 [08:51<06:30,  1.76s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 279/500 [08:53<06:28,  1.76s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 280/500 [08:54<06:26,  1.75s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 281/500 [08:56<06:24,  1.75s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 282/500 [08:58<06:22,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 283/500 [09:00<06:21,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 284/500 [09:01<06:19,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 285/500 [09:03<06:17,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 286/500 [09:05<06:15,  1.76s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 287/500 [09:07<06:13,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 288/500 [09:09<06:11,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 289/500 [09:10<06:09,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 290/500 [09:12<06:07,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 291/500 [09:14<06:05,  1.75s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 292/500 [09:15<06:03,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 293/500 [09:17<06:01,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 294/500 [09:19<06:00,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 295/500 [09:21<05:58,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 296/500 [09:22<05:56,  1.75s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 297/500 [09:24<05:54,  1.75s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 298/500 [09:26<05:52,  1.75s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 299/500 [09:28<05:50,  1.74s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 300/500 [09:29<05:49,  1.75s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 301/500 [09:31<05:47,  1.74s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 302/500 [09:33<05:45,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 303/500 [09:35<05:43,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 304/500 [09:36<05:41,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 305/500 [09:38<05:39,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 306/500 [09:40<05:38,  1.74s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 307/500 [09:42<05:36,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 308/500 [09:43<05:34,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 309/500 [09:45<05:32,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 310/500 [09:47<05:31,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 311/500 [09:49<05:29,  1.74s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 312/500 [09:50<05:27,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 313/500 [09:52<05:25,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 314/500 [09:54<05:24,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 315/500 [09:56<05:22,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 316/500 [09:57<05:20,  1.74s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 317/500 [09:59<05:18,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 318/500 [10:01<05:17,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 319/500 [10:03<05:15,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 320/500 [10:04<05:13,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 321/500 [10:06<05:12,  1.74s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 322/500 [10:08<05:10,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 323/500 [10:10<05:08,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 324/500 [10:11<05:06,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 325/500 [10:13<05:05,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 326/500 [10:15<05:03,  1.74s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 327/500 [10:17<05:01,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 328/500 [10:18<05:00,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 329/500 [10:20<04:58,  1.74s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 330/500 [10:22<04:56,  1.75s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 331/500 [10:24<04:54,  1.75s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 332/500 [10:25<04:53,  1.75s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 333/500 [10:27<04:51,  1.75s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 334/500 [10:29<04:49,  1.75s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 335/500 [10:30<04:48,  1.75s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 336/500 [10:32<04:46,  1.75s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 337/500 [10:34<04:44,  1.75s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 338/500 [10:36<04:43,  1.75s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 339/500 [10:37<04:41,  1.75s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 340/500 [10:39<04:39,  1.75s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 341/500 [10:41<04:38,  1.75s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 342/500 [10:43<04:36,  1.75s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 343/500 [10:45<04:34,  1.75s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 344/500 [10:46<04:33,  1.75s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 345/500 [10:48<04:31,  1.75s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 346/500 [10:50<04:29,  1.75s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 347/500 [10:52<04:28,  1.75s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 348/500 [10:53<04:26,  1.75s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 349/500 [10:55<04:25,  1.76s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 350/500 [10:57<04:23,  1.76s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 351/500 [10:59<04:21,  1.76s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 352/500 [11:00<04:20,  1.76s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 353/500 [11:02<04:18,  1.76s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 354/500 [11:04<04:17,  1.76s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 355/500 [11:06<04:15,  1.76s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 356/500 [11:07<04:13,  1.76s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 357/500 [11:09<04:12,  1.76s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 358/500 [11:11<04:10,  1.76s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 359/500 [11:13<04:08,  1.76s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 360/500 [11:14<04:07,  1.77s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 361/500 [11:16<04:05,  1.77s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 362/500 [11:18<04:04,  1.77s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 363/500 [11:20<04:02,  1.77s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 364/500 [11:22<04:00,  1.77s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 365/500 [11:23<03:59,  1.77s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 366/500 [11:25<03:57,  1.77s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 367/500 [11:27<03:55,  1.77s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 368/500 [11:29<03:54,  1.78s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 369/500 [11:30<03:52,  1.78s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 370/500 [11:32<03:51,  1.78s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 371/500 [11:34<03:49,  1.78s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 372/500 [11:36<03:47,  1.78s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 373/500 [11:38<03:45,  1.78s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 374/500 [11:39<03:43,  1.77s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 375/500 [11:41<03:41,  1.77s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 376/500 [11:43<03:39,  1.77s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 377/500 [11:45<03:38,  1.77s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 378/500 [11:46<03:36,  1.77s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 379/500 [11:48<03:34,  1.77s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 380/500 [11:50<03:32,  1.77s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 381/500 [11:52<03:31,  1.78s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 382/500 [11:53<03:29,  1.78s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 383/500 [11:55<03:28,  1.78s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 384/500 [11:57<03:26,  1.78s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 385/500 [11:59<03:24,  1.78s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 386/500 [12:01<03:23,  1.78s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 387/500 [12:02<03:21,  1.79s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 388/500 [12:04<03:20,  1.79s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 389/500 [12:06<03:18,  1.79s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 390/500 [12:08<03:17,  1.79s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 391/500 [12:10<03:15,  1.79s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 392/500 [12:11<03:14,  1.80s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 393/500 [12:13<03:12,  1.80s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 394/500 [12:15<03:09,  1.79s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 395/500 [12:17<03:07,  1.79s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 396/500 [12:19<03:06,  1.79s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 397/500 [12:20<03:04,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 398/500 [12:22<03:02,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 399/500 [12:24<03:01,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 400/500 [12:26<02:59,  1.79s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 401/500 [12:28<02:57,  1.80s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 402/500 [12:29<02:56,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 403/500 [12:31<02:54,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 404/500 [12:33<02:53,  1.80s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 405/500 [12:35<02:51,  1.81s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 406/500 [12:37<02:50,  1.81s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 407/500 [12:38<02:48,  1.81s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 408/500 [12:40<02:46,  1.81s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 409/500 [12:42<02:45,  1.82s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 410/500 [12:44<02:44,  1.82s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 411/500 [12:46<02:42,  1.83s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 412/500 [12:48<02:41,  1.83s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 413/500 [12:49<02:39,  1.83s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 414/500 [12:51<02:38,  1.84s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 415/500 [12:53<02:36,  1.84s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 416/500 [12:55<02:35,  1.85s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 417/500 [12:57<02:33,  1.85s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 418/500 [12:59<02:32,  1.86s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 419/500 [13:01<02:30,  1.86s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 420/500 [13:02<02:28,  1.86s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 421/500 [13:04<02:26,  1.86s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 422/500 [13:06<02:25,  1.86s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 423/500 [13:08<02:23,  1.86s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 424/500 [13:10<02:21,  1.86s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 425/500 [13:12<02:19,  1.87s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 426/500 [13:14<02:18,  1.87s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 427/500 [13:15<02:16,  1.87s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 428/500 [13:17<02:15,  1.88s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 429/500 [13:19<02:13,  1.88s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 430/500 [13:21<02:12,  1.89s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 431/500 [13:23<02:10,  1.89s/it]\u001b[A\n",
      "Iteration:  86%|████████▋ | 432/500 [13:25<02:08,  1.90s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 433/500 [13:27<02:07,  1.90s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 434/500 [13:29<02:05,  1.91s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 435/500 [13:31<02:04,  1.91s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 436/500 [13:33<02:02,  1.92s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 437/500 [13:35<02:01,  1.92s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 438/500 [13:37<01:59,  1.93s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 439/500 [13:38<01:57,  1.93s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 440/500 [13:40<01:56,  1.94s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 441/500 [13:42<01:54,  1.94s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 442/500 [13:44<01:53,  1.95s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 443/500 [13:46<01:51,  1.95s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 444/500 [13:48<01:49,  1.96s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 445/500 [13:50<01:48,  1.96s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 446/500 [13:52<01:46,  1.97s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 447/500 [13:54<01:44,  1.97s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 448/500 [13:56<01:42,  1.98s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 449/500 [13:58<01:41,  1.98s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 450/500 [14:00<01:39,  1.98s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 451/500 [14:02<01:37,  1.99s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 452/500 [14:04<01:35,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 453/500 [14:06<01:33,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 454/500 [14:08<01:31,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 455/500 [14:10<01:29,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 456/500 [14:12<01:27,  1.99s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 457/500 [14:14<01:25,  1.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 458/500 [14:16<01:23,  1.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 459/500 [14:18<01:21,  1.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 460/500 [14:20<01:19,  1.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 461/500 [14:22<01:17,  1.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 462/500 [14:24<01:15,  1.99s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 463/500 [14:26<01:13,  1.99s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 464/500 [14:28<01:11,  1.99s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 465/500 [14:30<01:09,  1.99s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 466/500 [14:32<01:07,  2.00s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 467/500 [14:34<01:05,  2.00s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 468/500 [14:36<01:04,  2.00s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 469/500 [14:38<01:01,  2.00s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 470/500 [14:40<00:59,  2.00s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 471/500 [14:42<00:57,  2.00s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 472/500 [14:44<00:56,  2.00s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 473/500 [14:46<00:54,  2.00s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 474/500 [14:48<00:51,  2.00s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 475/500 [14:50<00:49,  2.00s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 476/500 [14:52<00:47,  1.99s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 477/500 [14:54<00:45,  1.99s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 478/500 [14:56<00:43,  1.99s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 479/500 [14:58<00:41,  1.99s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 480/500 [15:00<00:39,  1.99s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 481/500 [15:02<00:37,  1.99s/it]\u001b[A\n",
      "Iteration:  96%|█████████▋| 482/500 [15:04<00:35,  1.99s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 483/500 [15:06<00:33,  1.99s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 484/500 [15:08<00:31,  1.99s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 485/500 [15:10<00:29,  1.98s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 486/500 [15:12<00:27,  1.98s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 487/500 [15:14<00:25,  1.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 488/500 [15:16<00:23,  1.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 489/500 [15:18<00:21,  1.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 490/500 [15:20<00:19,  1.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 491/500 [15:22<00:17,  1.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 492/500 [15:24<00:15,  1.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 493/500 [15:26<00:13,  1.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 494/500 [15:28<00:11,  1.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 495/500 [15:30<00:09,  1.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 496/500 [15:32<00:07,  1.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 497/500 [15:34<00:05,  1.98s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 498/500 [15:36<00:03,  1.98s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 499/500 [15:38<00:01,  1.98s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 500/500 [15:39<00:00,  1.88s/it]\u001b[A\n",
      "05/04/2023 03:01:27 - INFO - __main__ -   Loading features from cached file ./data/kmer4/train/cached_dev_4-new-12w-0_100_dnaprom\n",
      "05/04/2023 03:01:27 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/04/2023 03:01:27 - INFO - __main__ -     Num examples = 6555\n",
      "05/04/2023 03:01:27 - INFO - __main__ -     Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"learning_rate\": 2.962962962962963e-06, \"loss\": 0.4849108645319939, \"step\": 1100}\n",
      "{\"learning_rate\": 2.222222222222222e-06, \"loss\": 0.48343309581279753, \"step\": 1200}\n",
      "{\"learning_rate\": 1.4814814814814815e-06, \"loss\": 0.4859894973039627, \"step\": 1300}\n",
      "{\"learning_rate\": 7.407407407407407e-07, \"loss\": 0.48807529598474503, \"step\": 1400}\n",
      "{\"learning_rate\": 0.0, \"loss\": 0.48985504746437075, \"step\": 1500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 1/52 [00:00<00:34,  1.48it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 2/52 [00:01<00:35,  1.42it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 3/52 [00:02<00:34,  1.40it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 4/52 [00:02<00:33,  1.42it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 5/52 [00:03<00:33,  1.41it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 6/52 [00:04<00:32,  1.40it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 7/52 [00:04<00:31,  1.41it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 8/52 [00:05<00:31,  1.41it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 9/52 [00:06<00:30,  1.41it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 10/52 [00:07<00:29,  1.41it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 11/52 [00:07<00:29,  1.41it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 12/52 [00:08<00:28,  1.41it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 13/52 [00:09<00:27,  1.41it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 14/52 [00:09<00:26,  1.41it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 15/52 [00:10<00:26,  1.41it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 16/52 [00:11<00:25,  1.41it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 17/52 [00:12<00:24,  1.41it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 18/52 [00:12<00:24,  1.41it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 19/52 [00:13<00:23,  1.41it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 20/52 [00:14<00:22,  1.41it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 21/52 [00:14<00:21,  1.41it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 22/52 [00:15<00:21,  1.41it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 23/52 [00:16<00:20,  1.41it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 24/52 [00:17<00:19,  1.41it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 25/52 [00:17<00:19,  1.41it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 26/52 [00:18<00:18,  1.41it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 27/52 [00:19<00:17,  1.41it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 28/52 [00:19<00:16,  1.41it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 29/52 [00:20<00:16,  1.41it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 30/52 [00:21<00:15,  1.41it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 31/52 [00:21<00:14,  1.41it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 32/52 [00:22<00:14,  1.41it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 33/52 [00:23<00:13,  1.41it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 34/52 [00:24<00:12,  1.41it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 35/52 [00:24<00:12,  1.41it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 36/52 [00:25<00:11,  1.41it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 37/52 [00:26<00:10,  1.41it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 38/52 [00:26<00:09,  1.41it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 39/52 [00:27<00:09,  1.41it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 40/52 [00:28<00:08,  1.41it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 41/52 [00:29<00:07,  1.41it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 42/52 [00:29<00:07,  1.41it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 43/52 [00:30<00:06,  1.41it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 44/52 [00:31<00:05,  1.41it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 45/52 [00:31<00:04,  1.41it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 46/52 [00:32<00:04,  1.41it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 47/52 [00:33<00:03,  1.41it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 48/52 [00:33<00:02,  1.41it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 49/52 [00:34<00:02,  1.41it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 50/52 [00:35<00:01,  1.41it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 51/52 [00:36<00:00,  1.41it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 52/52 [00:36<00:00,  1.43it/s]\u001b[A\n",
      "05/04/2023 03:02:04 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     acc = 0.7746758199847444\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     auc = 0.810245762287683\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     f1 = 0.6074984125385676\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     mcc = 0.30562752766569146\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     precision = 0.5992698204595943\n",
      "05/04/2023 03:02:04 - INFO - __main__ -     recall = 0.7352381248263233\n",
      "Epoch: 100%|██████████| 3/3 [57:16<00:00, 1145.43s/it]\n",
      "05/04/2023 03:02:04 - INFO - __main__ -    global_step = 1500, average loss = 0.5179786022106806\n",
      "05/04/2023 03:02:04 - INFO - __main__ -   Saving model checkpoint to ./ft/kmer4/\n",
      "05/04/2023 03:02:04 - INFO - transformers.configuration_utils -   Configuration saved in ./ft/kmer4/config.json\n",
      "05/04/2023 03:02:04 - INFO - transformers.modeling_utils -   Model weights saved in ./ft/kmer4/pytorch_model.bin\n",
      "05/04/2023 03:02:04 - INFO - transformers.configuration_utils -   loading configuration file ./ft/kmer4/config.json\n",
      "05/04/2023 03:02:04 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 03:02:04 - INFO - transformers.modeling_utils -   loading weights file ./ft/kmer4/pytorch_model.bin\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   Model name './ft/kmer4/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './ft/kmer4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   Didn't find file ./ft/kmer4/added_tokens.json. We won't load it.\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/vocab.txt\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/special_tokens_map.json\n",
      "05/04/2023 03:02:06 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# activate conda env\n",
    "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
    "conda activate dnabert\n",
    "\n",
    "# running bert-pcp train script\n",
    "cd ./bert-pcp\n",
    "./train.sh ../DNABERT/examples/run_finetune.py ../4-new-12w-0/\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test w/ BERT-PCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1ZQVQUb4WJa",
    "outputId": "f9802981-f73e-4e84-dffd-fe5a9f3d5000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if the prediction directory is present\n",
      "prediction directory not found; making it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 03:02:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/04/2023 03:02:10 - INFO - transformers.configuration_utils -   loading configuration file ./ft/kmer4/config.json\n",
      "05/04/2023 03:02:10 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 03:02:10 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt from cache at /home/ec2-user/.cache/torch/transformers/7e2907c40805f9ae104ef14f1bb0e1d375c6edddace059a06037bdc80e8abe91.29586c1860b95c4db60b7024d64161b951279c64e0d9eeea911f286be8f229ae\n",
      "05/04/2023 03:02:10 - INFO - transformers.modeling_utils -   loading weights file ./ft/kmer4/pytorch_model.bin\n",
      "05/04/2023 03:02:12 - INFO - __main__ -   finish loading model\n",
      "05/04/2023 03:02:16 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='./data/kmer4/test/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=False, do_lower_case=False, do_predict=True, do_train=False, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=100, max_steps=-1, model_name_or_path='./ft/kmer4/', model_type='dna', n_gpu=1, n_process=32, no_cuda=False, num_rnn_layer=2, num_train_epochs=3.0, output_dir='./ft/kmer4/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_pred_batch_size=128, per_gpu_train_batch_size=8, predict_dir='./results/kmer4', predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna4', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0, warmup_steps=0, weight_decay=0.0)\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   Model name './ft/kmer4/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './ft/kmer4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   Didn't find file ./ft/kmer4/added_tokens.json. We won't load it.\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/vocab.txt\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/special_tokens_map.json\n",
      "05/04/2023 03:02:16 - INFO - transformers.tokenization_utils -   loading file ./ft/kmer4/tokenizer_config.json\n",
      "05/04/2023 03:02:16 - INFO - __main__ -   Predict using the following checkpoint: ./ft/kmer4/\n",
      "05/04/2023 03:02:16 - INFO - transformers.configuration_utils -   loading configuration file ./ft/kmer4/config.json\n",
      "05/04/2023 03:02:16 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 03:02:16 - INFO - transformers.modeling_utils -   loading weights file ./ft/kmer4/pytorch_model.bin\n",
      "05/04/2023 03:02:18 - INFO - __main__ -   Creating features from dataset file at ./data/kmer4/test/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "finish loading examples\n",
      "number of processes for converting feature: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1170\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 218 91 96 115 190 234 154 91 95 110 172 163 128 243 189 232 145 56 210 59 222 108 163 128 244 193 248 212 68 260 259 254 235 160 116 195 254 236 163 125 231 143 48 180 196 260 260 260 257 246 203 32 115 192 243 191 238 172 164 131 253 232 145 55 207 47 175 173 168 145 55 207 48 177 184 211 61 232 145 56 212 67 256 241 183 207 47 173 168 147 64 243 192 243 190 235 160 116 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 231 141 40 147 64 243 191 239 175 176 177 184 211 63 239 173 168 148 67 256 243 191 238 171 159 111 175 176 179 191 239 175 175 175 174 171 159 111 176 179 192 243 190 235 159 112 179 192 244 195 256 244 195 256 244 195 256 244 195 256 244 195 256 244 195 253 232 147 61 232 146 57 216 83 61 232 147 61 229 134 9 22 76 36 131 254 236 162 122 220 97 118 204 36 132 258 252 226 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3508\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 54 204 33 119 208 50 188 227 128 244 195 254 235 160 114 188 228 132 260 259 253 232 147 62 235 160 116 195 253 232 147 61 230 140 36 131 256 242 187 223 112 178 188 225 119 208 51 190 236 161 120 211 64 241 184 212 67 256 244 193 248 209 53 200 20 66 252 226 121 215 77 38 139 32 114 188 227 125 230 140 36 131 256 242 187 223 109 168 146 58 219 93 104 147 63 237 168 146 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1171\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 31 111 175 175 174 172 163 127 238 171 157 104 146 58 219 94 108 164 130 249 215 79 46 172 164 132 260 258 251 221 101 133 6 11 32 116 195 254 236 162 121 216 82 60 228 130 250 220 97 119 206 43 158 107 157 104 148 68 259 253 231 143 47 176 179 190 236 163 127 240 180 196 258 251 223 111 173 168 146 59 223 109 167 143 46 170 154 92 99 126 233 151 78 42 154 92 98 123 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 204 35 125 231 141 37 133 6 12 36 129 245 198 10 27 94 108 164 132 259 255 239 176 180 195 254 236 163 125 232 148 67 254 235 159 110 172 161 117 200 20 68 259 254 235 159 110 170 156 99 126 234 155 93 103 142 42 154 92 97 119 206 44 164 132 258 252 226 123 222 108 161 120 209 54 204 35 126 236 163 127 237 167 143 48 179 189 232 145 53 200 17 53 200 19 63 239 174 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1172\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 60 226 122 218 92 99 125 229 136 18 59 221 104 146 59 221 102 139 29 102 137 24 82 60 228 131 253 232 147 62 234 155 95 112 179 191 238 172 163 126 233 151 77 40 148 65 246 201 22 74 26 89 86 73 22 73 22 76 36 129 245 200 19 61 230 137 23 79 45 166 140 36 131 253 232 148 65 248 210 60 227 126 236 164 129 245 197 8 20 66 252 228 130 252 226 123 221 104 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-2339\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 67 255 239 175 174 171 159 111 173 168 148 67 254 235 158 108 161 120 210 59 222 107 159 111 176 180 195 254 236 163 125 232 148 67 256 244 193 246 204 36 129 246 204 36 132 260 259 254 234 155 94 106 155 93 104 148 67 256 244 194 252 228 131 256 244 195 253 232 147 61 232 147 64 241 181 200 20 66 252 228 131 256 244 195 256 244 195 253 232 147 61 232 147 64 244 195 253 232 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3509\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 58 219 96 115 191 239 176 178 186 219 95 110 172 164 131 255 238 172 161 119 207 47 175 175 173 167 143 45 165 136 20 67 255 239 173 166 137 23 79 48 179 189 232 146 57 216 84 67 254 235 159 110 171 160 116 196 259 254 236 163 127 239 175 174 171 160 116 194 250 220 97 119 205 37 134 12 36 130 251 222 107 159 109 168 148 65 246 204 36 130 251 222 107 158 105 151 79 45 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 236 162 123 222 108 164 131 256 242 188 226 124 227 125 231 144 51 190 233 152 82 60 226 123 223 109 167 141 39 141 39 144 50 188 226 124 227 128 242 188 228 132 259 254 235 158 108 164 132 258 252 227 127 239 174 172 164 131 256 243 192 244 195 255 240 180 195 253 231 144 51 191 239 173 166 140 36 132 260 260 259 255 240 180 196 260 257 246 204 35 127 240 180 196 260 259 256 242 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-4677\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 226 122 218 92 100 129 248 211 62 236 164 129 248 209 55 208 52 195 255 238 172 164 132 258 252 227 126 236 164 131 256 241 181 200 19 64 244 193 248 212 67 255 240 180 193 248 210 58 217 85 70 9 23 77 39 144 49 184 212 65 245 199 14 43 157 102 140 35 125 229 133 8 20 65 248 212 68 260 257 247 206 42 154 91 96 116 193 245 197 6 11 31 109 165 133 7 14 42 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-2340\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 240 177 181 200 19 62 236 164 131 254 233 151 78 41 149 72 20 68 257 245 199 14 42 156 100 132 257 248 212 65 246 203 31 111 173 167 143 46 171 157 104 147 63 238 170 155 96 114 188 225 119 206 41 152 82 60 225 120 212 66 252 227 128 243 189 229 133 7 16 51 191 239 176 177 184 210 58 218 90 91 95 111 174 172 164 130 252 227 128 243 192 244 196 258 250 219 95 112 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3510\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 30 106 154 92 97 118 204 33 120 212 68 259 253 232 145 56 211 62 235 157 103 144 50 186 220 99 125 230 138 28 97 117 200 17 55 208 49 181 197 7 15 46 171 160 116 196 260 257 248 212 66 251 221 104 148 67 256 243 190 236 162 123 222 106 154 91 95 110 170 155 95 111 174 171 159 111 174 172 163 126 235 160 116 195 256 244 195 254 235 159 109 167 143 45 167 141 40 146 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-2341\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 62 234 155 94 106 154 91 93 102 140 36 131 255 240 179 191 240 177 184 212 67 256 243 192 242 188 228 132 260 259 255 239 176 180 196 259 255 237 168 145 55 207 46 172 161 120 212 68 259 255 239 175 174 171 159 110 170 156 100 132 260 257 247 208 51 192 244 196 260 260 260 259 256 243 191 240 180 196 259 255 240 180 195 253 232 147 63 240 179 192 244 194 251 223 109 166 139 32 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1169\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1173\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 256 244 195 256 244 195 254 236 164 129 248 212 65 248 209 56 211 64 243 192 244 194 252 228 129 248 209 56 211 63 240 177 184 211 64 244 196 259 256 244 196 259 256 244 195 256 244 196 258 252 227 128 244 193 248 211 64 244 196 259 256 241 184 212 68 257 248 211 64 243 192 243 192 243 192 244 195 255 240 179 191 237 167 141 37 133 8 19 62 235 160 116 196 259 256 243 191 240 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3511\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 65 248 211 63 237 168 146 60 226 121 216 84 67 256 243 191 240 179 189 229 133 8 19 64 244 193 245 200 20 68 260 259 256 243 192 243 192 244 193 246 201 24 81 56 211 62 235 159 111 173 168 146 60 226 124 227 127 237 165 136 19 64 242 188 228 132 259 256 244 194 249 214 73 23 77 40 146 57 213 69 7 13 37 133 8 17 55 205 37 135 15 47 175 174 169 150 74 27 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-4678\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 56 211 63 240 180 193 245 200 17 55 207 47 174 171 158 107 158 106 154 91 96 115 190 236 162 122 218 92 97 120 209 56 210 59 222 107 158 107 160 116 195 254 235 157 101 136 20 65 247 207 48 180 196 257 248 212 66 249 213 72 17 56 212 66 250 218 92 100 132 257 247 206 44 163 127 239 175 176 180 195 253 229 135 14 43 159 109 168 148 68 258 252 226 123 222 108 164 130 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   Writing example 0/1175\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-3512\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 219 93 101 135 16 51 191 240 178 186 220 100 131 256 241 181 200 19 63 237 168 147 62 236 163 126 236 164 129 248 212 66 252 227 127 240 177 184 209 53 198 11 30 108 161 120 210 58 218 91 96 116 195 253 229 136 19 61 232 147 63 237 168 148 66 251 222 108 164 129 245 197 7 14 41 149 70 9 22 74 26 90 89 85 69 5 5 6 12 33 119 206 41 151 77 39 143 45 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   guid: dev-1174\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   input_ids: 2 53 197 5 8 19 64 244 194 251 221 101 135 15 47 173 168 147 63 237 167 144 49 182 204 35 128 241 184 211 63 237 165 133 8 20 65 246 202 27 94 106 156 100 131 254 235 159 109 165 136 19 63 238 172 164 130 251 223 110 172 164 131 254 236 162 122 217 88 82 58 218 92 100 129 245 199 14 43 157 102 140 35 125 231 143 47 173 168 145 54 204 35 126 233 149 71 14 43 3\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:20 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 232 145 55 206 44 163 126 235 158 108 164 131 253 232 147 63 240 180 193 248 209 56 212 65 248 212 67 256 244 194 252 227 128 244 195 256 244 194 252 228 131 256 243 190 236 163 128 244 193 248 209 55 207 47 176 180 194 251 223 109 168 145 55 208 51 191 238 172 164 131 256 244 195 255 240 179 191 240 180 195 253 231 141 39 141 37 136 20 67 256 243 190 234 154 91 94 105 152 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-5846\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 222 107 159 109 165 136 19 61 229 136 17 54 204 36 131 256 244 193 248 212 65 248 211 63 240 179 189 232 146 59 222 108 162 124 226 122 220 99 125 232 147 62 234 155 95 110 169 151 78 42 155 93 101 134 10 28 99 126 236 163 126 236 164 129 248 212 68 260 257 245 200 20 65 247 206 42 153 87 80 52 193 246 204 34 123 222 107 159 111 175 173 168 145 53 197 7 13 37 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-2342\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 159 110 171 158 108 164 132 258 249 215 79 45 165 135 14 43 158 105 150 74 28 99 128 243 189 232 147 62 235 160 115 190 236 163 127 240 178 188 227 128 242 186 218 89 85 71 15 47 173 168 148 67 256 241 184 212 65 248 212 65 248 212 65 248 212 65 248 209 53 197 5 6 10 27 95 111 175 175 173 168 145 54 202 27 96 116 196 259 253 232 148 67 255 239 176 179 189 231 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-5847\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 27 95 111 173 166 140 36 131 254 236 164 131 255 237 168 145 56 212 65 248 212 65 245 199 16 51 190 234 154 92 98 124 226 122 219 94 107 157 102 139 32 116 193 248 211 62 236 163 125 230 140 36 132 257 245 200 18 59 222 108 163 125 230 137 23 77 40 147 61 229 133 8 18 60 225 119 207 46 172 163 125 230 140 35 127 238 171 157 103 143 46 170 153 86 76 36 129 245 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-7015\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 155 94 106 154 92 100 132 259 254 235 160 116 196 260 260 259 254 235 159 111 176 180 193 248 211 61 232 148 68 259 256 241 184 209 56 211 62 235 160 115 192 242 187 224 115 191 240 180 193 245 197 8 20 65 245 200 17 55 208 52 196 257 245 200 17 53 197 8 20 68 259 253 232 148 67 256 244 195 254 235 160 116 195 256 244 196 259 256 242 187 222 106 155 94 107 159 109 167 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-8184\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 15 45 166 138 28 97 118 204 34 122 217 88 82 59 223 109 168 145 56 210 58 219 93 101 133 5 8 20 68 258 252 225 120 210 58 218 89 85 69 5 7 14 42 154 91 94 106 153 85 70 12 33 118 204 36 130 249 215 77 40 147 61 229 134 10 25 86 74 26 92 99 126 234 153 86 73 21 69 7 14 44 163 126 236 161 117 198 10 25 85 70 11 31 110 169 151 78 42 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-5848\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 19 61 232 145 54 201 24 84 66 250 218 91 93 104 145 56 209 54 201 21 69 8 17 56 210 60 225 117 197 7 15 45 165 133 5 5 7 13 39 143 48 179 189 231 141 39 143 46 171 157 102 139 31 109 165 133 6 11 30 108 163 128 243 190 235 157 102 137 23 78 42 156 99 125 232 147 61 229 133 6 12 34 122 218 92 99 126 236 162 123 221 104 146 58 220 99 126 234 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 12 34 122 220 98 122 220 97 118 201 22 75 31 110 172 164 132 260 259 255 237 167 143 47 176 177 184 210 58 217 85 69 8 20 68 260 260 260 257 245 197 6 11 31 112 180 196 260 260 259 254 236 163 126 236 163 125 231 143 47 176 179 191 237 167 143 48 180 194 251 222 108 164 132 259 255 239 173 168 147 63 240 180 196 260 260 259 255 239 175 175 174 172 162 121 216 82 59 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-4679\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 216 82 60 228 131 255 237 167 141 40 147 63 238 170 153 87 77 40 148 67 253 232 148 67 253 232 148 68 260 258 252 228 130 250 220 100 130 252 226 123 221 101 135 13 40 148 68 260 260 260 259 255 237 165 135 13 40 148 68 258 249 215 79 45 168 145 56 211 63 237 165 136 17 55 207 47 174 171 160 116 195 255 238 171 159 110 171 159 111 175 175 176 179 191 240 179 191 238 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-4680\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 7 15 46 170 153 88 83 61 229 136 17 54 204 36 131 256 244 195 254 235 159 111 174 172 164 132 259 256 242 187 223 111 174 172 163 128 243 191 238 172 164 130 251 224 115 191 239 175 175 173 166 140 34 124 228 129 245 198 12 36 132 259 256 242 185 214 75 29 104 148 68 260 259 253 230 139 31 109 166 139 32 115 191 238 172 164 132 258 252 227 128 244 195 253 232 146 60 228 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-7016\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 254 236 164 132 259 255 238 170 154 92 98 123 222 107 158 107 160 115 190 235 159 111 173 167 143 45 168 145 56 211 62 234 156 100 129 245 198 11 31 112 178 187 222 107 158 108 163 126 235 158 106 155 94 108 163 125 230 138 27 94 107 159 109 167 143 46 171 160 116 196 257 248 211 63 239 175 174 172 164 132 260 257 247 206 43 158 108 162 124 225 119 208 52 195 255 238 171 158 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-7017\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 248 209 56 212 67 255 240 180 195 256 244 194 252 228 132 257 248 211 64 241 184 212 68 258 251 224 116 196 258 249 216 82 57 216 81 56 209 56 210 60 227 128 244 194 252 227 127 239 173 167 141 40 148 67 255 238 172 163 125 232 147 64 244 196 258 251 223 109 165 133 7 15 46 171 157 104 148 67 255 239 175 175 173 168 147 63 239 176 178 188 227 127 240 179 190 234 155 96 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-4681\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 199 14 42 153 88 84 68 259 254 236 163 126 234 156 98 124 228 131 254 236 164 132 259 253 231 142 43 160 115 192 243 189 232 145 56 212 67 255 240 180 195 255 239 176 177 183 208 49 184 211 63 237 166 140 36 130 250 220 99 126 236 164 132 257 248 211 64 241 183 208 51 192 244 196 260 259 256 244 196 259 255 239 174 172 164 132 260 260 258 251 223 110 171 157 104 147 64 242 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-2343\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 42 154 91 94 106 154 91 95 109 165 136 18 58 217 87 80 52 195 256 243 189 229 136 18 58 219 94 108 163 128 244 195 256 243 190 235 160 116 193 248 211 63 238 171 159 111 174 170 156 99 128 241 182 203 31 111 173 167 144 52 195 255 240 180 196 257 247 206 44 163 127 239 176 180 193 248 210 60 227 125 230 140 36 132 259 256 243 192 244 196 259 255 237 168 148 68 257 247 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-8185\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 256 243 192 243 190 236 163 125 230 138 28 98 124 228 132 260 258 252 228 131 256 244 195 256 244 195 256 244 195 255 240 179 192 243 190 233 152 83 64 241 183 208 51 192 244 196 260 259 255 240 180 196 259 256 244 195 254 236 161 120 211 64 242 186 219 96 115 192 244 196 260 259 255 240 177 184 212 67 255 237 166 140 36 131 254 234 154 91 94 108 164 132 259 256 244 196 260 260 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-8186\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 168 148 68 259 255 238 170 155 94 108 164 129 245 198 12 33 119 206 43 157 104 145 53 197 6 12 33 118 204 36 130 250 220 100 129 246 204 33 117 198 11 31 111 174 172 162 121 213 70 10 26 92 98 124 225 117 197 8 18 59 222 105 152 82 58 217 86 74 26 89 85 70 10 28 98 121 213 71 13 40 146 57 213 70 9 21 69 8 18 57 215 77 38 139 31 110 169 151 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-5849\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 239 174 172 163 127 238 171 159 111 175 176 179 191 239 176 179 192 243 189 231 143 47 176 179 191 239 176 180 193 248 212 67 254 235 160 115 192 243 192 243 191 239 176 179 192 241 181 200 20 68 260 257 247 208 51 189 232 147 64 241 181 197 7 15 48 180 196 260 259 255 239 176 179 192 243 191 237 168 148 67 255 237 168 147 63 240 180 196 257 247 208 52 193 247 208 51 191 240 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   guid: dev-7018\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   input_ids: 2 58 220 100 129 247 206 42 156 100 132 260 257 248 212 68 259 254 236 163 128 243 189 229 133 8 20 67 254 236 163 127 240 180 196 257 248 211 62 236 164 132 257 245 200 19 63 239 176 180 195 256 243 192 243 192 242 187 224 114 186 218 92 99 126 236 164 132 260 259 254 236 162 122 218 92 98 124 227 128 242 186 220 99 126 236 163 126 236 162 124 227 126 233 151 79 48 179 192 3\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:21 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   guid: dev-7019\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   input_ids: 2 211 62 236 163 128 243 191 238 172 163 128 243 189 229 135 14 43 157 102 138 28 100 131 256 243 191 237 165 136 17 54 204 36 131 256 241 182 204 36 129 248 209 54 204 33 120 212 67 254 234 155 95 109 168 146 60 228 131 254 235 160 115 189 229 136 19 63 238 171 158 106 153 88 83 64 241 184 209 56 209 55 206 44 164 132 259 255 240 179 192 241 183 205 39 142 41 149 72 3\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   guid: dev-8187\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   input_ids: 2 22 75 31 112 179 191 238 172 164 132 259 256 242 187 221 103 144 51 190 234 155 96 114 188 228 132 260 259 256 244 196 257 247 208 49 184 212 65 248 209 53 200 19 63 237 165 133 7 16 50 185 213 69 8 17 55 205 39 143 45 168 148 65 248 210 58 218 91 94 107 160 116 196 259 255 239 173 168 147 62 236 162 124 228 131 254 236 163 126 236 163 127 240 180 196 260 257 248 3\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   guid: dev-5850\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   input_ids: 2 159 111 175 176 179 191 238 169 152 84 66 252 227 126 236 164 131 255 240 177 184 211 64 243 192 244 196 260 259 256 244 196 259 253 232 148 67 256 241 184 211 64 243 192 244 195 256 244 195 256 244 195 256 244 195 256 241 184 211 64 242 188 228 129 247 205 40 145 53 200 18 60 228 131 256 244 194 250 220 99 126 236 161 119 208 51 191 238 172 164 129 245 197 6 10 27 95 111 3\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   guid: dev-8188\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   input_ids: 2 113 184 211 63 239 176 179 190 234 154 91 95 109 168 148 68 257 247 207 47 174 169 151 79 46 172 161 120 212 68 259 255 239 173 167 141 40 148 66 252 225 120 212 67 253 232 147 63 238 172 164 131 255 238 169 152 83 61 232 148 67 255 239 175 173 167 144 51 191 237 167 143 48 179 191 238 171 158 108 163 127 238 171 159 109 168 148 67 255 240 179 191 239 176 179 190 236 163 3\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/04/2023 03:02:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "05/04/2023 03:07:45 - INFO - __main__ -   Saving features into cached file ./data/kmer4/test/cached_dev_100_dnaprom\n",
      "05/04/2023 03:07:46 - INFO - __main__ -   ***** Running prediction  *****\n",
      "05/04/2023 03:07:46 - INFO - __main__ -     Num examples = 9358\n",
      "05/04/2023 03:07:46 - INFO - __main__ -     Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processor started !\n",
      "2 processor started !\n",
      "3 processor started !\n",
      "4 processor started !\n",
      "5 processor started !\n",
      "6 processor started !\n",
      "7 processor started !\n",
      "8 processor started !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 74/74 [00:47<00:00,  1.56it/s]\n",
      "05/04/2023 03:08:33 - INFO - __main__ -   ***** Pred results  *****\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     acc = 0.7655481940585596\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     auc = 0.7856460185443317\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     f1 = 0.6053671651797384\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     mcc = 0.2914228609185937\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     precision = 0.5982293602304934\n",
      "05/04/2023 03:08:33 - INFO - __main__ -     recall = 0.7161453654658283\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# setting up dnabert environment\n",
    "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
    "conda activate dnabert\n",
    "\n",
    "# running bert-pcp test script - trained\n",
    "cd ./bert-pcp\n",
    "./test.sh ../DNABERT/examples/run_finetune.py 0\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTr16NnBfv5l",
    "outputId": "2608c41e-680c-43f0-e379-664bb5a1ee80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if the prediction directory is present\n",
      "prediction directory not found; making it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 03:08:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/04/2023 03:08:37 - INFO - transformers.configuration_utils -   loading configuration file ../4-new-12w-0/config.json\n",
      "05/04/2023 03:08:37 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 03:08:37 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt from cache at /home/ec2-user/.cache/torch/transformers/7e2907c40805f9ae104ef14f1bb0e1d375c6edddace059a06037bdc80e8abe91.29586c1860b95c4db60b7024d64161b951279c64e0d9eeea911f286be8f229ae\n",
      "05/04/2023 03:08:37 - INFO - transformers.modeling_utils -   loading weights file ../4-new-12w-0/pytorch_model.bin\n",
      "05/04/2023 03:08:39 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/04/2023 03:08:39 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "05/04/2023 03:08:39 - INFO - __main__ -   finish loading model\n",
      "05/04/2023 03:08:43 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='./data/kmer4/test/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=False, do_lower_case=False, do_predict=True, do_train=False, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=100, max_steps=-1, model_name_or_path='../4-new-12w-0/', model_type='dna', n_gpu=1, n_process=32, no_cuda=False, num_rnn_layer=2, num_train_epochs=3.0, output_dir='../4-new-12w-0/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_pred_batch_size=128, per_gpu_train_batch_size=8, predict_dir='./results/base_kmer4', predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna4', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0, warmup_steps=0, weight_decay=0.0)\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   Model name '../4-new-12w-0/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '../4-new-12w-0/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   Didn't find file ../4-new-12w-0/added_tokens.json. We won't load it.\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   loading file ../4-new-12w-0/vocab.txt\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   loading file ../4-new-12w-0/special_tokens_map.json\n",
      "05/04/2023 03:08:43 - INFO - transformers.tokenization_utils -   loading file ../4-new-12w-0/tokenizer_config.json\n",
      "05/04/2023 03:08:43 - INFO - __main__ -   Predict using the following checkpoint: ../4-new-12w-0/\n",
      "05/04/2023 03:08:43 - INFO - transformers.configuration_utils -   loading configuration file ../4-new-12w-0/config.json\n",
      "05/04/2023 03:08:43 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 261\n",
      "}\n",
      "\n",
      "05/04/2023 03:08:43 - INFO - transformers.modeling_utils -   loading weights file ../4-new-12w-0/pytorch_model.bin\n",
      "05/04/2023 03:08:45 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/04/2023 03:08:45 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "05/04/2023 03:08:45 - INFO - __main__ -   Loading features from cached file ./data/kmer4/test/cached_dev_100_dnaprom\n",
      "05/04/2023 03:08:46 - INFO - __main__ -   ***** Running prediction  *****\n",
      "05/04/2023 03:08:46 - INFO - __main__ -     Num examples = 9358\n",
      "05/04/2023 03:08:46 - INFO - __main__ -     Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 74/74 [00:46<00:00,  1.59it/s]\n",
      "05/04/2023 03:09:32 - INFO - __main__ -   ***** Pred results  *****\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     acc = 0.7365890147467408\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     auc = 0.6226272341480037\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     f1 = 0.5387200041881699\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     mcc = 0.12937431101628177\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     precision = 0.5446178367083052\n",
      "05/04/2023 03:09:32 - INFO - __main__ -     recall = 0.5937837509938151\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# setting up dnabert environment\n",
    "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
    "conda activate dnabert\n",
    "\n",
    "# running bert-pcp test script - base\n",
    "cd ./bert-pcp\n",
    "./test.sh ../DNABERT/examples/run_finetune.py 1\n",
    "\n",
    "conda deactivate"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
